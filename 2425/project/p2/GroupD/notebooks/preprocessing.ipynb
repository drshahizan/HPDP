# 1. Setup Spark Session
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, regexp_replace, trim, when, length
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover

spark = SparkSession.builder \
    .appName("SentimentPreprocessing") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# Set log level to WARN to reduce verbosity
spark.sparkContext.setLogLevel("WARN")

print("Spark Session initialized.")

# 2. Load Raw Data
# Assuming you have collected data into data/raw_data/grab_reviews_raw.csv
# If your CSV has headers, Spark can infer schema.
raw_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("data/raw_data/grab_reviews_raw.csv") # Adjust path if needed

print(f"Raw data loaded. Total records: {raw_df.count()}")
raw_df.printSchema()
raw_df.show(5, truncate=False)

# 3. Data Cleaning and Preprocessing Steps
# Select relevant columns and rename for consistency
df_processed = raw_df.select(
    col("review_id"),
    col("content").alias("text"), # Use 'content' from your producer output as the text for analysis
    col("score").alias("original_score"),
    col("timestamp"),
    col("repliedAt").alias("replied_at")
)

# Handle missing values: Drop rows where 'text' (review content) is null
# This is crucial as ML models cannot process null text.
df_processed = df_processed.dropna(subset=["text"])
print(f"Records after dropping null text: {df_processed.count()}")

# Basic Text Cleaning: Lowercasing and removing non-alphanumeric characters (keep spaces)
df_processed = df_processed.withColumn("text", lower(col("text")))
df_processed = df_processed.withColumn("text", regexp_replace(col("text"), "[^a-z\\s]", "")) # Remove non-alphabetic chars
df_processed = df_processed.withColumn("text", trim(col("text"))) # Trim leading/trailing spaces

# Remove empty strings after cleaning
df_processed = df_processed.filter(length(col("text")) > 0)
print(f"Records after cleaning empty text: {df_processed.count()}")

# Tokenization: Split text into words
tokenizer = Tokenizer(inputCol="text", outputCol="words")
words_data = tokenizer.transform(df_processed)
words_data.show(5, truncate=False)

# Remove Stop Words (Optional but recommended for sentiment analysis)
# Ensure you have the default stop words list or create a custom one
# PySpark's default StopWordsRemover uses a reasonable English list.
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
filtered_data = remover.transform(words_data)
filtered_data.show(5, truncate=False)

# Feature Hashing (Term Frequency - TF)
# This converts text (words) into numerical feature vectors.
hashing_tf = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=10000) # numFeatures can be tuned
featurized_data = hashing_tf.transform(filtered_data)
featurized_data.show(5, truncate=False)

# IDF (Inverse Document Frequency)
# This re-weights the TF features to account for term importance across documents.
idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(featurized_data)
final_features_df = idf_model.transform(featurized_data)
final_features_df.show(5, truncate=False)

# 4. Prepare for Model Training: Add a 'label' column based on 'original_score'
# Define a simple labeling scheme for sentiment for training purposes
# Example: score >= 4 is positive, score <= 2 is negative, score = 3 is neutral
# This needs to be refined based on your actual sentiment definition.
# For simplicity, let's create a binary classification for now (positive/negative)
# Or create a 3-class for positive, neutral, negative:
# 0: negative, 1: neutral, 2: positive

df_labeled = final_features_df.withColumn(
    "label",
    when(col("original_score") >= 4, 2.0)    # Positive
    .when(col("original_score") <= 2, 0.0)    # Negative
    .otherwise(1.0)                          # Neutral (score = 3)
)

print(f"Schema with new 'label' column:")
df_labeled.printSchema()
df_labeled.select("original_score", "label", "text").show(10, truncate=True)

# 5. Save the Cleaned and Featurized Data
# It's good practice to save this preprocessed data if you have large dataset,
# so you don't have to re-run preprocessing every time you train.
# Using Parquet is recommended for Spark as it's columnar and optimized.
output_cleaned_data_path = "data/cleaned_data.parquet"
df_labeled.write.mode("overwrite").parquet(output_cleaned_data_path)

print(f"Cleaned and featurized data saved to {output_cleaned_data_path}")
print("Preprocessing complete. You can now use 'cleaned_data.parquet' for model training.")

# Stop Spark Session
spark.stop()
