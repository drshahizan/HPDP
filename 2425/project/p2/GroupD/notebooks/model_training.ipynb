# 1. Setup Spark Session
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import StringIndexer, IndexToString

spark = SparkSession.builder \
    .appName("SentimentModelTraining") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# Set log level to WARN to reduce verbosity
spark.sparkContext.setLogLevel("WARN")

print("Spark Session initialized.")

# 2. Load Cleaned and Featurized Data
input_cleaned_data_path = "data/cleaned_data.parquet"
df_cleaned = spark.read.parquet(input_cleaned_data_path)

print(f"Cleaned data loaded. Total records: {df_cleaned.count()}")
df_cleaned.printSchema()
df_cleaned.select("review_id", "text", "original_score", "features", "label").show(5, truncate=False)

# 3. Prepare Data for Logistic Regression
# Logistic Regression expects 'features' and 'label' columns.
# We already created 'features' and 'label' in the preprocessing step.

# Convert numerical labels back to string labels for StringIndexer,
# which is useful for consistent labeling across training and prediction.
# Define original sentiment labels based on your labeling scheme in preprocessing.
# Ensure this matches the order of labels expected by StringIndexer after fitting.
# For 0:negative, 1:neutral, 2:positive
sentiment_labels = ['negative', 'neutral', 'positive'] # This order maps to 0.0, 1.0, 2.0

# StringIndexer to convert original_score-based labels (0, 1, 2) back to string representation if needed,
# or more commonly, to ensure prediction labels are handled correctly.
# However, for training LogisticRegression with numerical labels (0.0, 1.0, 2.0),
# you might not strictly need StringIndexer here if your 'label' column is already numeric.
# If your 'label' column comes from text (e.g., 'positive', 'negative'), then StringIndexer is crucial.
# Assuming 'label' is already numeric (0.0, 1.0, 2.0) from preprocessing, LR can use it directly.

# If you were to start with text labels, you'd use this:
# label_indexer = StringIndexer(inputCol="score_based_label", outputCol="label") # assuming 'score_based_label' is a string column

# 4. Split Data into Training and Testing Sets
(training_data, test_data) = df_cleaned.randomSplit([0.8, 0.2], seed=42)

print(f"Training data records: {training_data.count()}")
print(f"Test data records: {test_data.count()}")

# 5. Train the Logistic Regression Model
# Logistic Regression expects 'features' and 'label' columns.
lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10, regParam=0.3, elasticNetParam=0.8) # maxIter, regParam, elasticNetParam can be tuned

# Fit the model
print("Training Logistic Regression model...")
lr_model = lr.fit(training_data)
print("Logistic Regression model trained successfully.")

# Print the coefficients and intercept for the trained model
print(f"Coefficients: {lr_model.coefficientMatrix}")
print(f"Intercept: {lr_model.interceptVector}")

# 6. Make Predictions on the Test Data
print("Making predictions on test data...")
predictions = lr_model.transform(test_data)
predictions.select("review_id", "text", "original_score", "label", "prediction", "probability").show(10, truncate=False)

# 7. Evaluate the Model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Test Set Accuracy = {accuracy}")

# You can also evaluate other metrics like F1-score
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
f1_score = evaluator_f1.evaluate(predictions)
print(f"Test Set F1 Score = {f1_score}")

# 8. Create a Spark ML Pipeline for Deployment (Optional but Recommended)
# This bundles the feature transformation steps (Tokenizer, HashingTF, IDF) and the LR model
# into a single deployable unit. Your spark_sentiment_consumer.py already loads a PipelineModel.

# Re-create the feature transformers to include in the pipeline
# These should be the same as in preprocessing.ipynb
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover

tokenizer = Tokenizer(inputCol="text", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
hashing_tf = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=10000)
idf = IDF(inputCol="raw_features", outputCol="features")

# StringIndexer to map prediction index back to original string label (e.g., 0 -> "negative")
# Ensure the labels match the order used when 'label' was created in preprocessing (0:negative, 1:neutral, 2:positive)
index_to_string = IndexToString(inputCol="prediction", outputCol="predicted_sentiment", labels=sentiment_labels)


# Define the Pipeline stages
# Note: You don't include the 'label' creation step here as that's for training data.
# The pipeline takes raw text and outputs prediction/probability.
pipeline = Pipeline(stages=[
    tokenizer,
    remover,
    hashing_tf,
    idf,
    lr # Your trained Logistic Regression model
    # index_to_string # Add this if you want the PipelineModel to output the string label directly
])

# Fit the entire pipeline on the training data.
# This will train all stages (IDF and LR model) in one go.
# However, you already have a trained lr_model. You might want to save the `lr_model`
# and the `idf_model` separately, or refit the whole pipeline.
# For simplicity, let's assume you fit this pipeline from scratch here,
# or you can use your already trained `lr_model` and `idf_model` if you manually load them.

# Option 1: Fit a new pipeline (recommended if the structure is the same as inference)
# For the sentiment labels, you need the original string values if they were used to fit a StringIndexer.
# Since we converted to numeric in preprocessing, let's assume the labels list is sufficient for IndexToString.
print("Fitting pipeline for deployment...")
# For a full pipeline, you'd fit it to your df_cleaned or training_data if it includes original text
# If you created the model as: Pipeline(stages=[tokenizer, remover, hashing_tf, idf, lr])
# Then you would do: pipeline_model = pipeline.fit(df_cleaned)

# However, your current consumer loads a PipelineModel containing the LR model.
# Let's directly save the LR model, and for the pipeline,
# you could reconstruct it for inference if needed, using the saved LR model and IDF model.

# More typically, the pipeline used in the consumer is a "PipelineModel" that contains
# the fitted HashingTF (if it was fitted), IDFModel, and the LRModel.
# The `pipeline` object above is an unfitted Pipeline.

# If your consumer loads PipelineModel.load("lr_model_score_labeling"),
# then "lr_model_score_labeling" should be the result of a `pipeline.fit()` call.
# Let's create a Pipeline that you would fit to generate such a model.
# This pipeline should contain all the stages from feature engineering (except label creation)
# and the trained LR model.

pipeline_for_saving = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, lr_model]) # Use lr_model directly
# The pipeline model expects the raw features. It uses the fitted stages.
# The `lr_model` is already fitted.

# To save a full pipeline, you'd typically do:
# pipeline_for_saving = Pipeline(stages=[tokenizer, remover, hashing_tf, idf, lr]) # The unfitted LR estimator
# model_for_deployment = pipeline_for_saving.fit(training_data) # Fit the whole thing

# Let's assume the consumer expects the PipelineModel to output 'prediction' and 'probability'
# and then it adds the IndexToString stage.

# Re-evaluate your consumer: it says "Attempting to load Logistic Regression model from lr_model_score_labeling..."
# This implies it loads just the LR model, not a full pipeline model.
# If so, you just need to save the `lr_model`.

# 9. Save the Trained Model
# Model path: Ensure this matches the path your consumer loads from.
model_output_path = "lr_model_score_labeling"
lr_model.save(model_output_path)
print(f"Logistic Regression model saved to {model_output_path}")

# You might also want to save the IDF model if you use it separately for transformation later
# idf_model.save("idf_model_path")

# Stop Spark Session
spark.stop()
