{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Scrap 1000 Comments\n"
      ],
      "metadata": {
        "id": "1gPIdNgzpGSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gXzXtKzosWX"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-comment-downloader\n",
        "\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from youtube_comment_downloader import YoutubeCommentDownloader\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "API_KEY = 'AIzaSyB63OLkOL-9Ma0E7YomjiHIu1XEk0WkzvI'  # Replace with your API key\n",
        "SEARCH_QUERY = 'Hari Raya'\n",
        "MAX_COMMENTS = 1000\n",
        "RAW_CSV_FILE = 'youtube_comments.csv'\n",
        "\n",
        "# === STEP 1: Search YouTube for \"Hari Raya\" videos ===\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "search_response = youtube.search().list(\n",
        "    q=SEARCH_QUERY,\n",
        "    type='video',\n",
        "    part='id,snippet',\n",
        "    maxResults=10  # Fetch top 10 videos\n",
        ").execute()\n",
        "\n",
        "video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
        "print(f\"ðŸ” Found {len(video_ids)} videos for '{SEARCH_QUERY}'\")\n",
        "\n",
        "# === STEP 2: Download Comments ===\n",
        "downloader = YoutubeCommentDownloader()\n",
        "all_comments = []\n",
        "\n",
        "for vid in video_ids:\n",
        "    try:\n",
        "        url = f'https://www.youtube.com/watch?v={vid}'\n",
        "        print(f\"ðŸ“¥ Scraping comments from: {url}\")\n",
        "        comments = downloader.get_comments_from_url(url)\n",
        "        for comment in comments:\n",
        "            all_comments.append({'video_id': vid, 'comment': comment['text']})\n",
        "            if len(all_comments) >= MAX_COMMENTS:\n",
        "                break\n",
        "        time.sleep(1)  # Avoid being blocked\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error scraping video {vid}: {e}\")\n",
        "    if len(all_comments) >= MAX_COMMENTS:\n",
        "        break\n",
        "\n",
        "# === STEP 3: Save raw comments to CSV ===\n",
        "df = pd.DataFrame(all_comments[:MAX_COMMENTS])\n",
        "df.to_csv(RAW_CSV_FILE, index=False)\n",
        "print(f\"âœ… Saved {len(df)} raw comments to {RAW_CSV_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing : Cleaning"
      ],
      "metadata": {
        "id": "NpYMaCSkpKyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# === STEP 1: Load raw data ===\n",
        "df = pd.read_csv('youtube_comments.csv')\n",
        "\n",
        "# === STEP 2: Clean function ===\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)  # Remove symbols/emojis\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "# === STEP 3: Apply cleaning ===\n",
        "df['clean_comment'] = df['comment'].apply(clean_text)\n",
        "\n",
        "# === STEP 4: Drop empty cleaned comments ===\n",
        "df = df[df['clean_comment'].str.strip() != '']\n",
        "\n",
        "# === STEP 5: Save cleaned comments only ===\n",
        "df[['clean_comment']].to_csv('cleaned_youtube_comments.csv', index=False)\n",
        "print(\"âœ… Cleaned comments saved to cleaned_youtube_comments.csv\")\n"
      ],
      "metadata": {
        "id": "lYwXj4M6pOTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}