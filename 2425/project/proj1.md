<a href="https://github.com/drshahizan/HPDP/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/HPDP" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/network/members"><img src="https://img.shields.io/github/forks/drshahizan/HPDP" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/HPDP" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/issues"><img src="https://img.shields.io/github/issues/drshahizan/HPDP" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/HPDP?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2FHPDP&labelColor=%23d9e3f0&countColor=%23697689&style=flat)

# üìÑ Project 1: Optimizing High-Performance Data Processing for Large-Scale Web Crawlers

## Project Synopsis
This project introduces students to real-world applications of high-performance computing (HPC) in large-scale web data processing. Students will work collaboratively in diverse teams to design, develop, and optimize a web crawler capable of extracting a minimum of **100,000 structured records** from a **Malaysian website**. Emphasis is placed on the practical implementation of **multithreading, multiprocessing, and distributed processing** techniques to improve the efficiency and scalability of the system. 

Through hands-on experience, students will explore data collection, cleaning, transformation, and storage, while addressing technical challenges such as crawl delays, ethical scraping, and performance bottlenecks. The project culminates with a comprehensive performance evaluation comparing pre- and post-optimization results. Deliverables include a final technical report, source code, cleaned dataset, performance analysis, and a group presentation. 

This project aims to develop students' technical skills in high-performance computing, critical thinking in system optimization, and collaboration in diverse teams‚Äîkey competencies for data science professionals.

üìä **Weightage**: 15% of total course assessment  
üïì Project Duration: 4 Weeks
üìÖ **Submission Deadline**: **Friday, 16 May 2025**  
üì§ **Submission Format**: All final documents must be uploaded via e-learning and github  
üë• **Assignment Type**: **4 students per group (Max)**. 
> üìå Each group must include students from **different genders, races, or backgrounds** to encourage diversity and collaboration across cultures and perspectives.

## üß† **What Is This Project About?**

This project gives you the chance to **design, build, and optimize a high-performance data processing system** that collects information from the internet. You will create a **web crawler** to extract data from a **Malaysian website** and then process this data using **advanced computing techniques** to make the process faster and more efficient.

### üí° What makes this ‚Äúhigh-performance‚Äù?
You are expected to use computing methods that improve performance, such as:
- **Multithreading** ‚Äì running multiple tasks at the same time
- **Multiprocessing** ‚Äì using multiple CPU cores for different parts of the program
- **Distributed computing** ‚Äì spreading the task across several machines or processes (e.g., using Spark)

You will also **compare the performance before and after optimization**, and show what makes your design better.

## üéØ **Project Goals**

By the end of this project, you will be able to:
1. Build a **web crawler** to extract large amounts of data from a real-world Malaysian website.
2. Process and clean the data to prepare it for analysis or storage.
3. Apply **high-performance computing techniques** to improve the speed and efficiency of your solution.
4. Work as part of a **diverse, collaborative team**.
5. Evaluate your system‚Äôs performance and explain how it was improved.
6. Present your work professionally through reports and presentations.

## üìå **Project Requirements**

### ‚úÖ Minimum Technical Requirements:
- Crawl and collect **at least 100,000 records** from a single Malaysian website.
- Store the data in **CSV, JSON, or a structured database**.
- Clean and process the data (e.g., remove duplicates, standardize fields).
- Apply at least **two optimization techniques** to make your system faster and more efficient.
- Compare **before vs after optimization** using performance metrics.

### ‚úÖ Deliverables (What You Must Submit):
| No. | Item | Description |
|-----|------|-------------|
| 1 | **Final Report** | A complete document with background, methods, results, and discussion. Must be uploaded to Turnitin (PDF). |
| 2 | **Source Code** | Your crawler and processing pipeline, well organized and commented. Submit as GitHub link or ZIP file. |
| 3 | **Clean Dataset** | At least 100,000 valid, structured records in CSV/JSON/database. |
| 4 | **Performance Comparison** | Include charts or tables comparing performance before and after optimization. |
| 5 | **Presentation Slides** | 10-minute group presentation explaining your project. |

## üåê **Suggested Malaysian Websites for Crawling**

Choose only **ONE** website from the list below. You must clearly explain what type of data you will extract and confirm that the website can provide at least **100,000 records**.

| No. | Website | Domain | Example Data Fields |
|-----|---------|--------|----------------------|
| 1 | [JobStreet](https://www.jobstreet.com.my) | Job Search | Job title, company name, location, salary, job category |
| 2 | [Lazada](https://www.lazada.com.my) | E-commerce | Product name, price, category, seller, ratings |
| 3 | [Mudah.my](https://www.mudah.my) | Online Marketplace | Listing title, item type, price, region, seller |
| 4 | [PropertyGuru](https://www.propertyguru.com.my) | Real Estate | Property title, price, location, property type |
| 5 | [NST](https://www.nst.com.my) | News | Headline, publication date, section, article summary |
| 6 | [SoyaCincau](https://soyacincau.com) | Tech Reviews | Article title, brand, topic, publication date |
| 7 | [CARSOME](https://www.carsome.my) | Car Sales | Car model, year, price, mileage, dealer name |
| 8 | [MyFutureJobs](https://www.myfuturejobs.gov.my) | Government Job Portal | Job title, employer, salary, sector, region |
| 9 | [iProperty](https://www.iproperty.com.my) | Real Estate | Listing name, price, number of rooms, location |

> ‚úÖ Each group must get **approval from your lecturer** before starting the crawling process.

## üß≠ **Step-by-Step Project Guide**

### üìç **Week 1: Planning & Setup**
- Form a group of 4 (with diverse members).
- Choose a target website and identify the data you want to collect.
- Get lecturer approval.
- Design your crawler‚Äôs architecture and processing plan.
- Decide which libraries or frameworks to use (e.g., Scrapy, BeautifulSoup, Requests, Dask, Spark).

### üìç **Week 2: Crawler Development**
- Build your web crawler to collect real-time data.
- Respect robots.txt and do not overload the website.
- Save data progressively to avoid loss.
- Store in CSV, JSON, or a database.

### üìç **Week 3: Data Processing & Optimization**
- Clean and transform your data (e.g., fix missing values, remove duplicates).
- Apply at least **two optimization techniques** such as:
  - `threading`, `asyncio`
  - `multiprocessing`
  - `Spark` or `Dask` for large-scale data
- Record your performance (time, memory, CPU usage).

### üìç **Week 4: Reporting & Submission**
- Create your final report and slides.
- Compare your system‚Äôs performance before and after optimization.
- Submit:
  - ‚úÖ Report (via Turnitin)
  - ‚úÖ Code (via GitHub/ZIP)
  - ‚úÖ Dataset (with minimum 100,000 records)
  - ‚úÖ Performance analysis and presentation slides

## üìä **Performance Evaluation Criteria**
You must compare the following between your basic and optimized versions:
- Total processing time (in seconds/minutes)
- CPU and memory usage
- Number of records processed per second (throughput)
- Charts/graphs showing improvements

## üßæ **Assessment Rubric**

| Area | Details | Weight |
|------|---------|--------|
| Problem Understanding | Clear objectives, dataset identification, ethical crawling | 10% |
| System Design | Architecture planning, tool selection, modularity | 15% |
| Crawler & Processing Implementation | Functional, efficient, and able to collect ‚â•100,000 records | 25% |
| Optimization Techniques | Use of HPC methods and correctness of application | 20% |
| Performance Evaluation | Metrics comparison, accuracy of results, charts | 10% |
| Final Report | Clarity, completeness, professionalism | 10% |
| Group Presentation | Teamwork, explanation, timing, Q&A | 10% |
| **Total** |  | **100%** |

## ‚ùì **Need Help?**

Throughout the project, you may consult your lecturer during lab sessions or via email. If you encounter issues such as:
- Website not responding
- Incomplete data collection
- Performance too slow

‚Üí You are **encouraged to troubleshoot first as a team**, then seek help with clear questions and examples.


Certainly! Here's the updated version of the **Project Synopsis**, now including a reference section that introduces relevant GitHub repositories and describes their use in the context of the project:

---

## üìÑ **Project Synopsis**

**Title**: *Optimizing High-Performance Data Processing for Large-Scale Web Crawlers*  
**Course**: High-Performance Data Processing (HPDP)

### üß† **Synopsis**

This project introduces students to practical applications of high-performance computing (HPC) in the field of large-scale web data processing. Students will collaborate in diverse teams to design, implement, and optimize a modular web crawler capable of extracting at least **100,000 structured records** from a **Malaysian website**.

The project emphasizes the use of **parallel, concurrent, and distributed processing techniques**, including multithreading, multiprocessing, and big data frameworks such as Apache Spark or Dask, to enhance performance and scalability. Students are expected to collect, clean, and process large volumes of real-world data efficiently, while adhering to ethical standards of web data extraction.

The final deliverables include a cleaned dataset, source code, performance benchmarks (before and after optimization), a comprehensive technical report, and a group presentation. By completing this project, students will strengthen their skills in large-scale data engineering, system optimization, and team-based problem solving.

---

## üîó Recommended Repositories & Tools**

To support your implementation, you may refer to the following **GitHub repositories**, which provide sample code, libraries, and frameworks that are useful for completing your project tasks:

#### 1. **Web Scraping**
[GitHub Repository: python-web](https://github.com/drshahizan/python-web)  
This repository provides examples and tools for building web crawlers using Python libraries such as:
- `Requests` and `BeautifulSoup` for basic scraping
- `Scrapy` for scalable, asynchronous crawling
- Handling of pagination, HTTP headers, and response parsing
Students can explore different techniques and adapt existing scripts to suit their target Malaysian websites.

#### 2. **Big Data Processing**
[GitHub Repository: Python-big-data](https://github.com/drshahizan/Python-big-data)  
Big data processing involves the **systematic handling, transformation, and analysis of large datasets** that are too complex for traditional methods. This repository provides practical examples in:
- Data ingestion from large sources
- Cleaning and transformation using `Pandas`, `Dask`, and `PySpark`
- Batch processing and memory-efficient techniques

Students are encouraged to integrate these tools into their pipeline to **optimize data throughput and minimize execution time**.

---

Would you like this enhanced version to be added to the existing Word document as well? I can generate an updated copy with this included.

## ‚úÖ [Student Project Checklist & Timeline Tracker](p1_checklist.md)

## üßæ **[Sample Report Structure (Final Report)](p1_report.md)**

## üìÅ **[GitHub Folder Template submission](p1_github.md)**

## Contribution üõ†Ô∏è
Please create an [Issue](https://github.com/drshahizan/HPDP/issues) for any improvements, suggestions or errors in the content.

You can also contact me using [Linkedin](https://www.linkedin.com/in/drshahizan/) for any other queries or feedback.

[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan)
![](https://hit.yhype.me/github/profile?user_id=81284918)
