{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Packages**"
      ],
      "metadata": {
        "id": "d6tM9RnDrjex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exZw5qPKE7ry",
        "outputId": "d735f576-7d1e-4d6c-aee6-33c7b97b2866",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supabase\n",
            "  Downloading supabase-2.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.3-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.4)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.11.18 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (4.13.2)\n",
            "Collecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest<1.1,>0.19->supabase) (24.2)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.0)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.5.0,>=2.4.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
            "Downloading supabase-2.15.1-py3-none-any.whl (17 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.3-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strenum, websockets, deprecation, pytest-mock, aiohttp, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "Successfully installed aiohttp-3.11.18 deprecation-2.1.0 gotrue-2.12.0 postgrest-1.0.1 pytest-mock-3.14.0 realtime-2.4.3 storage3-0.11.3 strenum-0.4.15 supabase-2.15.1 supafunc-0.9.4 websockets-14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install supabase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "h_Tl0m8grpL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from joblib import Parallel, delayed\n",
        "from prettytable import PrettyTable\n",
        "from supabase import create_client, Client"
      ],
      "metadata": {
        "id": "Nf-AzKGgFC8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Supabase Client**"
      ],
      "metadata": {
        "id": "-7B33pFor_54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ugjwigpcopmtjgylopwf.supabase.co\"\n",
        "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVnandpZ3Bjb3BtdGpneWxvcHdmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDU4MjgxMjIsImV4cCI6MjA2MTQwNDEyMn0.oFcP1wCt1upByqTU8NgD4FpJUdv9I8sG1ECWMX1wz8I\"\n",
        "\n",
        "supabase: Client = create_client(url, key)"
      ],
      "metadata": {
        "id": "scGC6KHjr74N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Fetching Function**"
      ],
      "metadata": {
        "id": "f5L2FQRXsE2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_car_data():\n",
        "    response = supabase.table(\"cars_clean\").select(\"*\").range(0, 999).execute()\n",
        "    batch_size = 1000\n",
        "    offset = 1000\n",
        "\n",
        "    while True:\n",
        "        batch_response = supabase.table(\"cars_clean\").select(\"*\").range(offset, offset + batch_size - 1).execute()\n",
        "        rows = batch_response.data\n",
        "        if not rows:\n",
        "            break\n",
        "        response.data.extend(rows)\n",
        "        offset += batch_size\n",
        "        print(f\"Fetched {len(response.data)} rows so far...\")\n",
        "\n",
        "    print(f\"âœ… Done. Total rows fetched: {len(response.data)}\")\n",
        "    return response.data"
      ],
      "metadata": {
        "id": "kV-RX96psH6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ’° Query 1: Most Expensive Car in Each Location & Record Query Performance**"
      ],
      "metadata": {
        "id": "mWSLc2cfsLCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: Most Expensive Car by Location\n",
        "def query_most_expensive_car_by_location(data, n_jobs=-1):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use numpy for balanced chunking\n",
        "    chunks = np.array_split(data, n_jobs)\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        most_expensive_cars = {}\n",
        "        for car in chunk:\n",
        "            location = car.get(\"c_location\")\n",
        "            price = car.get(\"c_price\")\n",
        "\n",
        "            # Skip invalid or noisy location values\n",
        "            if not location or location in [\"Used\", \"c_location\"]:\n",
        "                continue\n",
        "\n",
        "            if price is None or not isinstance(price, (int, float)):\n",
        "                continue\n",
        "\n",
        "            if location not in most_expensive_cars or price > most_expensive_cars[location][\"price\"]:\n",
        "                most_expensive_cars[location] = {\n",
        "                    \"price\": price,\n",
        "                    \"c_name\": car.get(\"c_name\"),\n",
        "                    \"id\": car.get(\"id\")\n",
        "                }\n",
        "        return most_expensive_cars\n",
        "\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
        "\n",
        "    most_expensive_cars = {}\n",
        "    for result in results:\n",
        "        for location, car_info in result.items():\n",
        "            # If location already exists, keep the one with higher price\n",
        "            if (location not in most_expensive_cars) or (car_info[\"price\"] > most_expensive_cars[location][\"price\"]):\n",
        "                most_expensive_cars[location] = car_info\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = end_time - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    throughput = len(data) / query_time if query_time > 0 else 0\n",
        "\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"ID\", \"Location\", \"Car Name\", \"Price\"]\n",
        "    for location, car_info in most_expensive_cars.items():\n",
        "        table.add_row([car_info[\"id\"], location, car_info[\"c_name\"], car_info[\"price\"]])\n",
        "\n",
        "    return table, query_time, cpu_percent, memory_info.percent, throughput"
      ],
      "metadata": {
        "id": "EWXWg2pJtG7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ“† Query 2: Total Number of Cars Available by Year & Record Query Performance**\n",
        "*(Shows Top 5)*"
      ],
      "metadata": {
        "id": "pKecAfVptH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 2: Total Cars per Year\n",
        "def query_total_cars_per_year(data, n_jobs=-1):\n",
        "    start_time = time.time()\n",
        "    # Use numpy for balanced chunking\n",
        "    chunks = np.array_split(data, n_jobs)\n",
        "    def process_chunk(chunk):\n",
        "        year_count = defaultdict(int)\n",
        "        for item in chunk:\n",
        "            year = item.get('c_year')\n",
        "            if year:\n",
        "                year_count[year] += 1\n",
        "        return year_count\n",
        "\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
        "    combined = defaultdict(int)\n",
        "    for result in results:\n",
        "        for year, count in result.items():\n",
        "            combined[year] += count\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = end_time - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    throughput = len(data) / query_time if query_time > 0 else 0\n",
        "\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Year\", \"Count\"]\n",
        "    for year, count in sorted(combined.items(), key=lambda x: x[0])[:5]:\n",
        "        table.add_row([year, count])\n",
        "\n",
        "    return table, query_time, cpu_percent, memory_info.percent, throughput"
      ],
      "metadata": {
        "id": "xEG31SPCtOwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸš— Query 3: Average Price of Cars by Engine Size (Grouped by 500cc) & Record Query Performance**\n",
        "*(Shows Top 5)*"
      ],
      "metadata": {
        "id": "Rpu7q-EMtPuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 3: Average Price by Engine Size (Grouped by 500cc Intervals, Top 5)\n",
        "def query_average_price_by_engine_size(data, n_jobs=-1):\n",
        "    start_time = time.time()\n",
        "    interval = 500  # Group size in cc\n",
        "\n",
        "    # Split the data into chunks for parallel processing\n",
        "    chunks = np.array_split(data, n_jobs)\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        engine_price_map = defaultdict(list)\n",
        "        for item in chunk:\n",
        "            engine = item.get('c_engine')\n",
        "            price = item.get('c_price')\n",
        "            if engine and isinstance(engine, (int, float)) and isinstance(price, (int, float)):\n",
        "                # Group by engine size intervals (500cc)\n",
        "                engine_group = int(engine // interval * interval)\n",
        "                engine_price_map[engine_group].append(price)\n",
        "        return engine_price_map\n",
        "\n",
        "    # Parallel processing\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
        "\n",
        "    # Combine results\n",
        "    combined = defaultdict(list)\n",
        "    for result in results:\n",
        "        for engine_group, prices in result.items():\n",
        "            combined[engine_group].extend(prices)\n",
        "\n",
        "    # Compute average prices\n",
        "    average_prices = {\n",
        "        group: sum(prices) / len(prices)\n",
        "        for group, prices in combined.items()\n",
        "        if prices\n",
        "    }\n",
        "\n",
        "    # Sort by average price descending and keep top 5\n",
        "    top5_avg_prices = sorted(average_prices.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = end_time - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    throughput = len(data) / query_time if query_time > 0 else 0\n",
        "\n",
        "\n",
        "    # Prepare table\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Engine Size Group (cc)\", \"Average Price\"]\n",
        "    for group, avg_price in top5_avg_prices:\n",
        "        table.add_row([f\"{group}cc\", f\"${avg_price:,.2f}\"])\n",
        "\n",
        "    return table, query_time, cpu_percent, memory_info.percent, throughput"
      ],
      "metadata": {
        "id": "YUgg3Dw5t54n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ“ Query 4: Total Number of Cars by Location & Record Query Performance**"
      ],
      "metadata": {
        "id": "J8LwtKk9t_uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 4: Total Cars by Location\n",
        "def query_total_cars_by_location(data, n_jobs=-1):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use numpy to split data into chunks for parallel processing\n",
        "    chunks = np.array_split(data, n_jobs)\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        location_counts = defaultdict(int)\n",
        "        for item in chunk:\n",
        "            location = item.get('c_location')\n",
        "            # Filter: exclude None, \"Used\", and \"c_location\"\n",
        "            if location and location != \"Used\" and location != \"c_location\":\n",
        "                location_counts[location] += 1\n",
        "        return location_counts\n",
        "\n",
        "    # Run parallel processing\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
        "\n",
        "    # Combine results\n",
        "    combined = defaultdict(int)\n",
        "    for result in results:\n",
        "        for location, count in result.items():\n",
        "            combined[location] += count\n",
        "\n",
        "    # Sort locations alphabetically\n",
        "    sorted_locations = sorted(combined.items())\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = end_time - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    throughput = len(data) / query_time if query_time > 0 else 0\n",
        "\n",
        "    # Create output table\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Location\", \"Count\"]\n",
        "    for location, count in sorted_locations:\n",
        "        table.add_row([location, count])\n",
        "\n",
        "    return table, query_time, cpu_percent, memory_info.percent, throughput"
      ],
      "metadata": {
        "id": "sOVV5fuouDav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ›£ï¸ Query 5: Average Minimum Mileage Grouped by Condition & Record Query Performance**"
      ],
      "metadata": {
        "id": "4TyWuk4QuHAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 5: Average Minimum Mileage by Condition\n",
        "def query_avg_min_mileage_by_condition(data, n_jobs=-1):\n",
        "    start_time = time.time()\n",
        "    # Use numpy for balanced chunking\n",
        "    chunks = np.array_split(data, n_jobs)\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        mileage_sums = defaultdict(float)\n",
        "        mileage_counts = defaultdict(int)\n",
        "        for car in chunk:\n",
        "            condition = car.get(\"c_condition\")\n",
        "            mileage = car.get(\"c_mileage_min\")\n",
        "            if condition and isinstance(mileage, (int, float)):\n",
        "                mileage_sums[condition] += mileage\n",
        "                mileage_counts[condition] += 1\n",
        "        return mileage_sums, mileage_counts\n",
        "\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
        "    total_sums = defaultdict(float)\n",
        "    total_counts = defaultdict(int)\n",
        "    for mileage_sums, mileage_counts in results:\n",
        "        for condition in mileage_sums:\n",
        "            total_sums[condition] += mileage_sums[condition]\n",
        "            total_counts[condition] += mileage_counts[condition]\n",
        "\n",
        "    avg_min_mileage_by_condition = {\n",
        "        condition: total_sums[condition] / total_counts[condition]\n",
        "        for condition in total_sums\n",
        "    }\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = end_time - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    throughput = len(data) / query_time if query_time > 0 else 0\n",
        "\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Condition\", \"Avg Min Mileage\"]\n",
        "    for condition, mileage in avg_min_mileage_by_condition.items():\n",
        "        table.add_row([condition, f\"{mileage:.2f}\"])\n",
        "\n",
        "    return table, query_time, cpu_percent, memory_info.percent, throughput"
      ],
      "metadata": {
        "id": "77BpAx8MuKk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Function to Run All Queries**"
      ],
      "metadata": {
        "id": "esHfiTzaupyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all queries\n",
        "def run_all_queries(data, n_jobs=4):\n",
        "    queries = [\n",
        "        (\"Query 1: Most Expensive Car by Location\", query_most_expensive_car_by_location),\n",
        "        (\"Query 2: Total Cars per Year\", query_total_cars_per_year),\n",
        "        (\"Query 3: Avg Price by Engine Size\", query_average_price_by_engine_size),\n",
        "        (\"Query 4: Total Cars by Location\", query_total_cars_by_location),\n",
        "        (\"Query 5: Avg Min Mileage by Condition\", query_avg_min_mileage_by_condition)\n",
        "    ]\n",
        "\n",
        "    for title, query_func in queries:\n",
        "        print(f\"\\n{title}\")\n",
        "        table, time_taken, cpu, mem, throughput = query_func(data, n_jobs)\n",
        "        print(table)\n",
        "        print(\"\\nQuery Performance: \")\n",
        "        print(f\"Time: {time_taken:.2f}s | CPU: {cpu}% | Memory: {mem}% | Throughput: {throughput:.2f} records/s\\n\")"
      ],
      "metadata": {
        "id": "eIS_npLZuowb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute Data Fetching and Run Queries**"
      ],
      "metadata": {
        "id": "f4QKMNusukk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data = fetch_car_data()\n",
        "    run_all_queries(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oeD-wY1FB4S",
        "outputId": "ac1fbc9c-51cb-43f8-ca00-7dbc226dcdc8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 2000 rows so far...\n",
            "Fetched 3000 rows so far...\n",
            "Fetched 4000 rows so far...\n",
            "Fetched 5000 rows so far...\n",
            "Fetched 6000 rows so far...\n",
            "Fetched 7000 rows so far...\n",
            "Fetched 8000 rows so far...\n",
            "Fetched 9000 rows so far...\n",
            "Fetched 10000 rows so far...\n",
            "Fetched 11000 rows so far...\n",
            "Fetched 12000 rows so far...\n",
            "Fetched 13000 rows so far...\n",
            "Fetched 14000 rows so far...\n",
            "Fetched 15000 rows so far...\n",
            "Fetched 16000 rows so far...\n",
            "Fetched 17000 rows so far...\n",
            "Fetched 18000 rows so far...\n",
            "Fetched 19000 rows so far...\n",
            "Fetched 20000 rows so far...\n",
            "Fetched 21000 rows so far...\n",
            "Fetched 22000 rows so far...\n",
            "Fetched 23000 rows so far...\n",
            "Fetched 24000 rows so far...\n",
            "Fetched 25000 rows so far...\n",
            "Fetched 26000 rows so far...\n",
            "Fetched 27000 rows so far...\n",
            "Fetched 28000 rows so far...\n",
            "Fetched 29000 rows so far...\n",
            "Fetched 30000 rows so far...\n",
            "Fetched 31000 rows so far...\n",
            "Fetched 32000 rows so far...\n",
            "Fetched 33000 rows so far...\n",
            "Fetched 34000 rows so far...\n",
            "Fetched 35000 rows so far...\n",
            "Fetched 36000 rows so far...\n",
            "Fetched 37000 rows so far...\n",
            "Fetched 38000 rows so far...\n",
            "Fetched 39000 rows so far...\n",
            "Fetched 40000 rows so far...\n",
            "Fetched 41000 rows so far...\n",
            "Fetched 42000 rows so far...\n",
            "Fetched 43000 rows so far...\n",
            "Fetched 44000 rows so far...\n",
            "Fetched 45000 rows so far...\n",
            "Fetched 46000 rows so far...\n",
            "Fetched 47000 rows so far...\n",
            "Fetched 48000 rows so far...\n",
            "Fetched 49000 rows so far...\n",
            "Fetched 50000 rows so far...\n",
            "Fetched 51000 rows so far...\n",
            "Fetched 52000 rows so far...\n",
            "Fetched 53000 rows so far...\n",
            "Fetched 54000 rows so far...\n",
            "Fetched 55000 rows so far...\n",
            "Fetched 56000 rows so far...\n",
            "Fetched 57000 rows so far...\n",
            "Fetched 58000 rows so far...\n",
            "Fetched 59000 rows so far...\n",
            "Fetched 60000 rows so far...\n",
            "Fetched 61000 rows so far...\n",
            "Fetched 62000 rows so far...\n",
            "Fetched 63000 rows so far...\n",
            "Fetched 64000 rows so far...\n",
            "Fetched 65000 rows so far...\n",
            "Fetched 66000 rows so far...\n",
            "Fetched 67000 rows so far...\n",
            "Fetched 68000 rows so far...\n",
            "Fetched 69000 rows so far...\n",
            "Fetched 70000 rows so far...\n",
            "Fetched 71000 rows so far...\n",
            "Fetched 72000 rows so far...\n",
            "Fetched 73000 rows so far...\n",
            "Fetched 74000 rows so far...\n",
            "Fetched 75000 rows so far...\n",
            "Fetched 76000 rows so far...\n",
            "Fetched 77000 rows so far...\n",
            "Fetched 78000 rows so far...\n",
            "Fetched 79000 rows so far...\n",
            "Fetched 80000 rows so far...\n",
            "Fetched 81000 rows so far...\n",
            "Fetched 82000 rows so far...\n",
            "Fetched 83000 rows so far...\n",
            "Fetched 84000 rows so far...\n",
            "Fetched 85000 rows so far...\n",
            "Fetched 86000 rows so far...\n",
            "Fetched 87000 rows so far...\n",
            "Fetched 88000 rows so far...\n",
            "Fetched 89000 rows so far...\n",
            "Fetched 90000 rows so far...\n",
            "Fetched 91000 rows so far...\n",
            "Fetched 92000 rows so far...\n",
            "Fetched 93000 rows so far...\n",
            "Fetched 94000 rows so far...\n",
            "Fetched 95000 rows so far...\n",
            "Fetched 96000 rows so far...\n",
            "Fetched 97000 rows so far...\n",
            "Fetched 98000 rows so far...\n",
            "Fetched 99000 rows so far...\n",
            "Fetched 100000 rows so far...\n",
            "Fetched 101000 rows so far...\n",
            "Fetched 102000 rows so far...\n",
            "Fetched 103000 rows so far...\n",
            "Fetched 104000 rows so far...\n",
            "Fetched 105000 rows so far...\n",
            "Fetched 106000 rows so far...\n",
            "Fetched 107000 rows so far...\n",
            "Fetched 108000 rows so far...\n",
            "Fetched 109000 rows so far...\n",
            "Fetched 110000 rows so far...\n",
            "Fetched 111000 rows so far...\n",
            "Fetched 112000 rows so far...\n",
            "Fetched 113000 rows so far...\n",
            "Fetched 114000 rows so far...\n",
            "Fetched 115000 rows so far...\n",
            "Fetched 115001 rows so far...\n",
            "âœ… Done. Total rows fetched: 115001\n",
            "\n",
            "Query 1: Most Expensive Car by Location\n",
            "+--------+-----------------+-----------------------------------------------+---------+\n",
            "|   ID   |     Location    |                    Car Name                   |  Price  |\n",
            "+--------+-----------------+-----------------------------------------------+---------+\n",
            "| 114269 |      Sabah      |          Lexus LX500d 3.3 F SPORT (A)         |  889000 |\n",
            "|  4697  |      Kedah      | Mclaren 720S 765LT SPIDER 1 OF 765 READY UNIT | 1880000 |\n",
            "| 88400  |      Johor      | Ferrari SF90 STRADALE 3.9 *1000hp Ready Stock | 4180000 |\n",
            "| 86014  |   Kuala Lumpur  |       Rolls Royce PHANTOM  6.75 V12 EWB       | 4900000 |\n",
            "|  6984  |     Selangor    |       Ferrari 812 Competizione Rosso TRS      | 8988000 |\n",
            "|  2327  |    Terengganu   |          Honda CIVIC 1.8 S i-VTEC (A)         | 2700000 |\n",
            "| 90583  |      Perak      |         Porsche 992 3.0 CARRERA S (A)         |  828000 |\n",
            "| 87864  |      Pahang     |  Mercedes Benz S63 4.0 AMG COUPE V8 BiTurbo S |  707000 |\n",
            "| 88258  | Negeri Sembilan |         Ferrari F12 BERLINETTA 6.3 (A)        | 1010000 |\n",
            "| 111306 |      Penang     | Rolls Royce CULLINAN 6.75 V12 (A) BLACK BADGE | 3498888 |\n",
            "| 117166 |     Kelantan    |  Land Rover RANGE ROVER 3.0 Big Spec P400 HST |  548000 |\n",
            "| 90503  |     Sarawak     |   Ferrari 488 Pista 3.9 V8 Twin-turbocharged  | 2660000 |\n",
            "| 92133  |      Melaka     |       Porsche 911 3.8 CARRERA S 991 (A)       |  489000 |\n",
            "| 111030 |    Putrajaya    | Mercedes Benz G350 D 3.0 AMG (A) S/ROOF UNREG |  568000 |\n",
            "| 118043 |      Labuan     |  Hyundai GRAND STAREX EXECUTIVE PLUS 2.5L (A) |  125000 |\n",
            "| 86503  |      Perlis     |   Bmw 340i xDRIVE M SPORT PRO M 3.0 MY19 G20  |  343800 |\n",
            "+--------+-----------------+-----------------------------------------------+---------+\n",
            "\n",
            "Query Performance: \n",
            "Time: 1.34s | CPU: 2.5% | Memory: 10.9% | Throughput: 85671.76 records/s\n",
            "\n",
            "\n",
            "Query 2: Total Cars per Year\n",
            "+------+-------+\n",
            "| Year | Count |\n",
            "+------+-------+\n",
            "| 1995 |  1349 |\n",
            "| 1996 |  374  |\n",
            "| 1997 |  493  |\n",
            "| 1998 |  188  |\n",
            "| 1999 |  318  |\n",
            "+------+-------+\n",
            "\n",
            "Query Performance: \n",
            "Time: 0.40s | CPU: 3.5% | Memory: 10.9% | Throughput: 287575.11 records/s\n",
            "\n",
            "\n",
            "Query 3: Avg Price by Engine Size\n",
            "+------------------------+---------------+\n",
            "| Engine Size Group (cc) | Average Price |\n",
            "+------------------------+---------------+\n",
            "|         6500cc         | $2,350,916.15 |\n",
            "|         6000cc         | $2,230,549.17 |\n",
            "|        99500cc         | $1,184,000.00 |\n",
            "|         3500cc         |  $924,417.12  |\n",
            "|         5000cc         |  $825,514.12  |\n",
            "+------------------------+---------------+\n",
            "\n",
            "Query Performance: \n",
            "Time: 0.44s | CPU: 14.6% | Memory: 11.3% | Throughput: 264270.50 records/s\n",
            "\n",
            "\n",
            "Query 4: Total Cars by Location\n",
            "+-----------------+-------+\n",
            "|     Location    | Count |\n",
            "+-----------------+-------+\n",
            "|      Johor      | 18904 |\n",
            "|      Kedah      |  3245 |\n",
            "|     Kelantan    |  1441 |\n",
            "|   Kuala Lumpur  | 31232 |\n",
            "|      Labuan     |   33  |\n",
            "|      Melaka     |  1647 |\n",
            "| Negeri Sembilan |  1552 |\n",
            "|      Pahang     |  1509 |\n",
            "|      Penang     |  6312 |\n",
            "|      Perak      |  5107 |\n",
            "|      Perlis     |  143  |\n",
            "|    Putrajaya    |  197  |\n",
            "|      Sabah      |  3440 |\n",
            "|     Sarawak     |  2268 |\n",
            "|     Selangor    | 36908 |\n",
            "|    Terengganu   |  1063 |\n",
            "+-----------------+-------+\n",
            "\n",
            "Query Performance: \n",
            "Time: 0.65s | CPU: 57.8% | Memory: 11.1% | Throughput: 177178.63 records/s\n",
            "\n",
            "\n",
            "Query 5: Avg Min Mileage by Condition\n",
            "+-----------+-----------------+\n",
            "| Condition | Avg Min Mileage |\n",
            "+-----------+-----------------+\n",
            "|    Used   |     99596.60    |\n",
            "|    New    |     3906.98     |\n",
            "|   Recon   |     21719.84    |\n",
            "+-----------+-----------------+\n",
            "\n",
            "Query Performance: \n",
            "Time: 0.56s | CPU: 3.0% | Memory: 11.2% | Throughput: 206451.23 records/s\n",
            "\n"
          ]
        }
      ]
    }
  ]
}