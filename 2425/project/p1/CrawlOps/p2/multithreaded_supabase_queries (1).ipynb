{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75100532",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75100532",
        "outputId": "c1a29d11-07c5-466c-ad89-9e1e0ed47ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supabase\n",
            "  Downloading supabase-2.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.3-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.4)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.11.18 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (4.13.2)\n",
            "Collecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.18->realtime<2.5.0,>=2.4.0->supabase) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest<1.1,>0.19->supabase) (24.2)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.0)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.5.0,>=2.4.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
            "Downloading supabase-2.15.1-py3-none-any.whl (17 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.3-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strenum, websockets, deprecation, pytest-mock, aiohttp, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "Successfully installed aiohttp-3.11.18 deprecation-2.1.0 gotrue-2.12.0 postgrest-1.0.1 pytest-mock-3.14.0 realtime-2.4.3 storage3-0.11.3 strenum-0.4.15 supabase-2.15.1 supafunc-0.9.4 websockets-14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install supabase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc346e3c",
      "metadata": {
        "id": "bc346e3c"
      },
      "source": [
        "# Multithreaded Supabase Data Processing\n",
        "\n",
        "This script demonstrates how to optimize the execution of analytical functions using multithreading.\n",
        "It first fetches all data from Supabase in a single operation, then uses multithreading to process\n",
        "the data concurrently using Pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e4e3783",
      "metadata": {
        "id": "5e4e3783"
      },
      "source": [
        "## I. Setup and Supabase Connection\n",
        "\n",
        "First, we'll import all necessary libraries and initialize our Supabase client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd3af76",
      "metadata": {
        "id": "bfd3af76"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psutil\n",
        "import concurrent.futures\n",
        "from supabase import create_client, Client\n",
        "from typing import Dict, Any, List, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51db8c2",
      "metadata": {
        "id": "b51db8c2"
      },
      "outputs": [],
      "source": [
        "# Initialize Supabase client\n",
        "SUPABASE_URL = \"https://ugjwigpcopmtjgylopwf.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVnandpZ3Bjb3BtdGpneWxvcHdmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDU4MjgxMjIsImV4cCI6MjA2MTQwNDEyMn0.oFcP1wCt1upByqTU8NgD4FpJUdv9I8sG1ECWMX1wz8I\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e6229a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "73e6229a"
      },
      "outputs": [],
      "source": [
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b127322d",
      "metadata": {
        "id": "b127322d"
      },
      "source": [
        "## II. Global Data Fetching and Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6a8819",
      "metadata": {
        "id": "dc6a8819"
      },
      "outputs": [],
      "source": [
        "def fetch_all_data_from_supabase(client: Client) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Fetches all data from the cars_clean table in Supabase using pagination.\n",
        "\n",
        "    Args:\n",
        "        client: The Supabase client instance\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing all fetched data, or None if an error occurs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parameters for pagination\n",
        "        page_size = 1000\n",
        "        start_range = 0\n",
        "\n",
        "        # List to store all fetched rows\n",
        "        all_data = []\n",
        "        cumulative_row_count = 0\n",
        "\n",
        "        # Continue fetching until no more data is returned\n",
        "        while True:\n",
        "            # Fetch a batch of data using range pagination\n",
        "            response = client.table(\"cars_clean\") \\\n",
        "                .select(\"*\") \\\n",
        "                .range(start_range, start_range + page_size - 1) \\\n",
        "                .execute()\n",
        "\n",
        "            # Get the current batch of data\n",
        "            batch_data = response.data\n",
        "\n",
        "            # If no data is returned, we've reached the end\n",
        "            if not batch_data:\n",
        "                break\n",
        "\n",
        "            # Add the batch to our collected data\n",
        "            all_data.extend(batch_data)\n",
        "\n",
        "            # Update the count and display progress\n",
        "            cumulative_row_count += len(batch_data)\n",
        "            print(f\"Fetched {cumulative_row_count} rows so far...\")\n",
        "\n",
        "            # Move to the next page\n",
        "            start_range += page_size\n",
        "\n",
        "        # Create a DataFrame from all collected data\n",
        "        df = pd.DataFrame(all_data)\n",
        "\n",
        "        # Print final summary\n",
        "        print(f\"✅ Done. Total rows fetched: {len(df)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from Supabase: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375a0089",
      "metadata": {
        "id": "375a0089"
      },
      "outputs": [],
      "source": [
        "def execute_with_metrics(func):\n",
        "    \"\"\"Decorator to add performance metrics to processing functions\"\"\"\n",
        "    def wrapper(*args, **kwargs):  # Modified to accept any arguments\n",
        "        # Record start metrics\n",
        "        start_time = time.time()\n",
        "        start_cpu = psutil.cpu_percent(interval=None)\n",
        "        start_memory = psutil.virtual_memory().percent\n",
        "\n",
        "        # Execute function\n",
        "        result_df = func(*args, **kwargs)\n",
        "\n",
        "        # Record end metrics\n",
        "        end_time = time.time()\n",
        "        end_cpu = psutil.cpu_percent(interval=None)\n",
        "        end_memory = psutil.virtual_memory().percent\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        duration = end_time - start_time\n",
        "        avg_cpu = (start_cpu + end_cpu) / 2\n",
        "        avg_memory = (start_memory + end_memory) / 2\n",
        "\n",
        "        # Modified Throughput Calculation (using input DataFrame length)\n",
        "        input_all_data_df = args[0]  # Access the first positional argument\n",
        "        throughput = len(input_all_data_df) / duration if duration > 0 else 0\n",
        "\n",
        "        return {\n",
        "            \"data\": result_df,\n",
        "            \"metrics\": {\n",
        "                \"duration\": duration,\n",
        "                \"cpu_percent\": avg_cpu,\n",
        "                \"memory_percent\": avg_memory,\n",
        "                \"throughput\": throughput,  # Using the modified throughput\n",
        "                \"result_count\": len(result_df)\n",
        "            }\n",
        "        }\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e63afb",
      "metadata": {
        "id": "10e63afb"
      },
      "outputs": [],
      "source": [
        "@execute_with_metrics\n",
        "def process_most_expensive_car_per_location(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Process 1: Find the most expensive car (name and price) in each distinct c_location.\n",
        "    Returns DataFrame with columns: c_location, c_name, c_price\n",
        "    \"\"\"\n",
        "    # Filter to relevant columns\n",
        "    df = all_data_df[['c_location', 'c_name', 'c_price']]\n",
        "\n",
        "    # Find most expensive car per location\n",
        "    result_df = df.loc[df.groupby('c_location')['c_price'].idxmax()]\n",
        "    result_df = result_df[['c_location', 'c_name', 'c_price']].sort_values('c_location')\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a88504c",
      "metadata": {
        "id": "8a88504c"
      },
      "outputs": [],
      "source": [
        "@execute_with_metrics\n",
        "def process_total_cars_per_year(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Process 2: Calculate the total number of cars available for each c_year.\n",
        "    Returns DataFrame with columns: c_year, total_cars, limited to top 5 earliest years.\n",
        "    \"\"\"\n",
        "    # Filter to relevant column\n",
        "    df = all_data_df[['c_year']]\n",
        "\n",
        "    # Count cars per year\n",
        "    result_df = df.groupby('c_year').size().reset_index(name='total_cars')\n",
        "\n",
        "    # Sort by year and limit to the top 5 earliest years\n",
        "    result_df = result_df.sort_values('c_year').head(5)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823963e2",
      "metadata": {
        "id": "823963e2"
      },
      "outputs": [],
      "source": [
        "@execute_with_metrics\n",
        "def process_average_price_by_engine_group(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Process 3: Calculate the average c_price grouped by c_engine size in 500cc intervals.\n",
        "    Returns DataFrame with columns: engine_group_start_cc, average_price (rounded to 2 decimal places),\n",
        "    limited to top 5 engine groups with highest average prices.\n",
        "    \"\"\"\n",
        "    # Filter to relevant columns\n",
        "    df = all_data_df[['c_engine', 'c_price']]\n",
        "\n",
        "    # Create engine group intervals (500cc each)\n",
        "    df['engine_group_start_cc'] = (df['c_engine'] // 500) * 500\n",
        "\n",
        "    # Calculate average price per engine group\n",
        "    result_df = df.groupby('engine_group_start_cc')['c_price'].mean().reset_index()\n",
        "\n",
        "    # Round average_price to 2 decimal places\n",
        "    result_df = result_df.rename(columns={'c_price': 'average_price'})\n",
        "    result_df['average_price'] = result_df['average_price'].round(2)\n",
        "\n",
        "    # Sort by average_price in descending order and limit to top 5\n",
        "    result_df = result_df.sort_values('average_price', ascending=False).head(5)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1fce44",
      "metadata": {
        "id": "5f1fce44"
      },
      "outputs": [],
      "source": [
        "@execute_with_metrics\n",
        "def process_total_cars_by_location(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Process 4: Calculate the total number of cars for each distinct c_location.\n",
        "    Returns DataFrame with columns: c_location, total_cars\n",
        "    \"\"\"\n",
        "    # Filter to relevant column\n",
        "    df = all_data_df[['c_location']]\n",
        "\n",
        "    # Count cars per location\n",
        "    result_df = df.groupby('c_location').size().reset_index(name='total_cars')\n",
        "    result_df = result_df.sort_values('c_location')\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59c73e4",
      "metadata": {
        "id": "b59c73e4"
      },
      "outputs": [],
      "source": [
        "@execute_with_metrics\n",
        "def process_average_mileage_by_condition(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Process 5: Calculate the average c_mileage_min for each car c_condition.\n",
        "    Returns DataFrame with columns: c_condition, average_min_mileage\n",
        "    \"\"\"\n",
        "    # Filter to relevant columns\n",
        "    df = all_data_df[['c_condition', 'c_mileage_min']]\n",
        "\n",
        "    # Calculate average mileage per condition\n",
        "    result_df = df.groupby('c_condition')['c_mileage_min'].mean().reset_index()\n",
        "    result_df = result_df.rename(columns={'c_mileage_min': 'average_min_mileage'})\n",
        "    result_df = result_df.sort_values('c_condition')\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d583bfe",
      "metadata": {
        "id": "9d583bfe"
      },
      "source": [
        "## III. Output Display and Multithreaded Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b36f7cd",
      "metadata": {
        "id": "1b36f7cd"
      },
      "outputs": [],
      "source": [
        "def display_query_output(query_title, results):\n",
        "    \"\"\"\n",
        "    Display query output in a formatted way\n",
        "\n",
        "    Args:\n",
        "        query_title: The title of the query\n",
        "        results: Dictionary containing data and metrics returned by the execute_with_metrics decorator\n",
        "    \"\"\"\n",
        "    metrics = results['metrics']\n",
        "    data = results['data']\n",
        "\n",
        "    print(f\"\\n[{query_title}]\")\n",
        "    print(data.to_string(index=False))\n",
        "\n",
        "    print(\"\\nQuery Performance:\")\n",
        "    print(f\"  Query Time: {metrics['duration']:.4f} seconds\")\n",
        "    print(f\"  Average CPU Usage: {metrics['cpu_percent']:.2f}%\")\n",
        "    print(f\"  Average Memory Usage: {metrics['memory_percent']:.2f}%\")\n",
        "    print(f\"  Throughput: {metrics['throughput']:.2f} records/second\")\n",
        "    print(\"--------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f5770b",
      "metadata": {
        "id": "69f5770b"
      },
      "outputs": [],
      "source": [
        "def run_multithreaded(all_data_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Execute all data processing functions using multithreading\n",
        "\n",
        "    Args:\n",
        "        all_data_df: The pre-fetched DataFrame containing all data\n",
        "    \"\"\"\n",
        "    print(\"\\n===== MULTITHREADED DATA PROCESSING =====\\n\")\n",
        "\n",
        "    # Define processing functions and their titles in a fixed, ordered list\n",
        "    processing_functions_list = [\n",
        "        (\"Query 1: Most Expensive by Location\", process_most_expensive_car_per_location),\n",
        "        (\"Query 2: Total Cars Per Year\", process_total_cars_per_year),\n",
        "        (\"Query 3: Average Price By Engine Group\", process_average_price_by_engine_group),\n",
        "        (\"Query 4: Total Cars By Location\", process_total_cars_by_location),\n",
        "        (\"Query 5: Average Mileage By Condition\", process_average_mileage_by_condition)\n",
        "    ]\n",
        "\n",
        "    # Convert to dictionary for ThreadPoolExecutor\n",
        "    processing_functions = dict(processing_functions_list)\n",
        "\n",
        "    # Record start time for multithreaded execution\n",
        "    multithreaded_start = time.time()\n",
        "\n",
        "    # Execute processing functions concurrently using ThreadPoolExecutor\n",
        "    results = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        # Submit all processing functions to the executor with the pre-fetched data\n",
        "        future_to_query = {executor.submit(func, all_data_df): title\n",
        "                          for title, func in processing_functions.items()}\n",
        "\n",
        "        # Collect results as they complete\n",
        "        for future in concurrent.futures.as_completed(future_to_query):\n",
        "            query_title = future_to_query[future]\n",
        "            try:\n",
        "                results[query_title] = future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"{query_title} generated an exception: {e}\")\n",
        "\n",
        "    # Record end time and calculate total duration\n",
        "    multithreaded_end = time.time()\n",
        "    multithreaded_duration = multithreaded_end - multithreaded_start\n",
        "\n",
        "    # Display results in the predefined order\n",
        "    for query_title, _ in processing_functions_list:\n",
        "        if query_title in results:\n",
        "            display_query_output(query_title, results[query_title])\n",
        "        else:\n",
        "            print(f\"\\n[{query_title}]\")\n",
        "            print(\"Error: This query did not complete successfully.\")\n",
        "            print(\"--------------------------------------------------\")\n",
        "\n",
        "    print(f\"\\n===== MULTITHREADED PROCESSING COMPLETED =====\")\n",
        "    print(f\"Total Execution Time: {multithreaded_duration:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a9e898",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a9e898",
        "outputId": "78e55196-8cf9-4f8b-a266-3d004bfc974f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching all data from Supabase...\n",
            "Fetched 1000 rows so far...\n",
            "Fetched 2000 rows so far...\n",
            "Fetched 3000 rows so far...\n",
            "Fetched 4000 rows so far...\n",
            "Fetched 5000 rows so far...\n",
            "Fetched 6000 rows so far...\n",
            "Fetched 7000 rows so far...\n",
            "Fetched 8000 rows so far...\n",
            "Fetched 9000 rows so far...\n",
            "Fetched 10000 rows so far...\n",
            "Fetched 11000 rows so far...\n",
            "Fetched 12000 rows so far...\n",
            "Fetched 13000 rows so far...\n",
            "Fetched 14000 rows so far...\n",
            "Fetched 15000 rows so far...\n",
            "Fetched 16000 rows so far...\n",
            "Fetched 17000 rows so far...\n",
            "Fetched 18000 rows so far...\n",
            "Fetched 19000 rows so far...\n",
            "Fetched 20000 rows so far...\n",
            "Fetched 21000 rows so far...\n",
            "Fetched 22000 rows so far...\n",
            "Fetched 23000 rows so far...\n",
            "Fetched 24000 rows so far...\n",
            "Fetched 25000 rows so far...\n",
            "Fetched 26000 rows so far...\n",
            "Fetched 27000 rows so far...\n",
            "Fetched 28000 rows so far...\n",
            "Fetched 29000 rows so far...\n",
            "Fetched 30000 rows so far...\n",
            "Fetched 31000 rows so far...\n",
            "Fetched 32000 rows so far...\n",
            "Fetched 33000 rows so far...\n",
            "Fetched 34000 rows so far...\n",
            "Fetched 35000 rows so far...\n",
            "Fetched 36000 rows so far...\n",
            "Fetched 37000 rows so far...\n",
            "Fetched 38000 rows so far...\n",
            "Fetched 39000 rows so far...\n",
            "Fetched 40000 rows so far...\n",
            "Fetched 41000 rows so far...\n",
            "Fetched 42000 rows so far...\n",
            "Fetched 43000 rows so far...\n",
            "Fetched 44000 rows so far...\n",
            "Fetched 45000 rows so far...\n",
            "Fetched 46000 rows so far...\n",
            "Fetched 47000 rows so far...\n",
            "Fetched 48000 rows so far...\n",
            "Fetched 49000 rows so far...\n",
            "Fetched 50000 rows so far...\n",
            "Fetched 51000 rows so far...\n",
            "Fetched 52000 rows so far...\n",
            "Fetched 53000 rows so far...\n",
            "Fetched 54000 rows so far...\n",
            "Fetched 55000 rows so far...\n",
            "Fetched 56000 rows so far...\n",
            "Fetched 57000 rows so far...\n",
            "Fetched 58000 rows so far...\n",
            "Fetched 59000 rows so far...\n",
            "Fetched 60000 rows so far...\n",
            "Fetched 61000 rows so far...\n",
            "Fetched 62000 rows so far...\n",
            "Fetched 63000 rows so far...\n",
            "Fetched 64000 rows so far...\n",
            "Fetched 65000 rows so far...\n",
            "Fetched 66000 rows so far...\n",
            "Fetched 67000 rows so far...\n",
            "Fetched 68000 rows so far...\n",
            "Fetched 69000 rows so far...\n",
            "Fetched 70000 rows so far...\n",
            "Fetched 71000 rows so far...\n",
            "Fetched 72000 rows so far...\n",
            "Fetched 73000 rows so far...\n",
            "Fetched 74000 rows so far...\n",
            "Fetched 75000 rows so far...\n",
            "Fetched 76000 rows so far...\n",
            "Fetched 77000 rows so far...\n",
            "Fetched 78000 rows so far...\n",
            "Fetched 79000 rows so far...\n",
            "Fetched 80000 rows so far...\n",
            "Fetched 81000 rows so far...\n",
            "Fetched 82000 rows so far...\n",
            "Fetched 83000 rows so far...\n",
            "Fetched 84000 rows so far...\n",
            "Fetched 85000 rows so far...\n",
            "Fetched 86000 rows so far...\n",
            "Fetched 87000 rows so far...\n",
            "Fetched 88000 rows so far...\n",
            "Fetched 89000 rows so far...\n",
            "Fetched 90000 rows so far...\n",
            "Fetched 91000 rows so far...\n",
            "Fetched 92000 rows so far...\n",
            "Fetched 93000 rows so far...\n",
            "Fetched 94000 rows so far...\n",
            "Fetched 95000 rows so far...\n",
            "Fetched 96000 rows so far...\n",
            "Fetched 97000 rows so far...\n",
            "Fetched 98000 rows so far...\n",
            "Fetched 99000 rows so far...\n",
            "Fetched 100000 rows so far...\n",
            "Fetched 101000 rows so far...\n",
            "Fetched 102000 rows so far...\n",
            "Fetched 103000 rows so far...\n",
            "Fetched 104000 rows so far...\n",
            "Fetched 105000 rows so far...\n",
            "Fetched 106000 rows so far...\n",
            "Fetched 107000 rows so far...\n",
            "Fetched 108000 rows so far...\n",
            "Fetched 109000 rows so far...\n",
            "Fetched 110000 rows so far...\n",
            "Fetched 111000 rows so far...\n",
            "Fetched 112000 rows so far...\n",
            "Fetched 113000 rows so far...\n",
            "Fetched 114000 rows so far...\n",
            "Fetched 115000 rows so far...\n",
            "Fetched 115001 rows so far...\n",
            "✅ Done. Total rows fetched: 115001\n",
            "Total records processed: 115001\n",
            "\n",
            "===== MULTITHREADED DATA PROCESSING =====\n",
            "\n",
            "\n",
            "[Query 1: Most Expensive by Location]\n",
            "     c_location                                        c_name  c_price\n",
            "          Johor Ferrari SF90 STRADALE 3.9 *1000hp Ready Stock  4180000\n",
            "          Kedah Mclaren 720S 765LT SPIDER 1 OF 765 READY UNIT  1880000\n",
            "       Kelantan  Land Rover RANGE ROVER 3.0 Big Spec P400 HST   548000\n",
            "   Kuala Lumpur             Rolls Royce PHANTOM  6.75 V12 EWB  4900000\n",
            "         Labuan  Hyundai GRAND STAREX EXECUTIVE PLUS 2.5L (A)   125000\n",
            "         Melaka             Porsche 911 3.8 CARRERA S 991 (A)   489000\n",
            "Negeri Sembilan                Ferrari F12 BERLINETTA 6.3 (A)  1010000\n",
            "         Pahang  Mercedes Benz S63 4.0 AMG COUPE V8 BiTurbo S   707000\n",
            "         Penang Rolls Royce CULLINAN 6.75 V12 (A) BLACK BADGE  3498888\n",
            "          Perak                 Porsche 992 3.0 CARRERA S (A)   828000\n",
            "         Perlis    Bmw 340i xDRIVE M SPORT PRO M 3.0 MY19 G20   343800\n",
            "      Putrajaya Mercedes Benz G350 D 3.0 AMG (A) S/ROOF UNREG   568000\n",
            "          Sabah                  Lexus LX500d 3.3 F SPORT (A)   889000\n",
            "        Sarawak    Ferrari 488 Pista 3.9 V8 Twin-turbocharged  2660000\n",
            "       Selangor            Ferrari 812 Competizione Rosso TRS  8988000\n",
            "     Terengganu                  Honda CIVIC 1.8 S i-VTEC (A)  2700000\n",
            "\n",
            "Query Performance:\n",
            "  Query Time: 0.0796 seconds\n",
            "  Average CPU Usage: 56.10%\n",
            "  Average Memory Usage: 10.40%\n",
            "  Throughput: 1444293.20 records/second\n",
            "--------------------------------------------------\n",
            "\n",
            "[Query 2: Total Cars Per Year]\n",
            " c_year  total_cars\n",
            "   1995        1349\n",
            "   1996         374\n",
            "   1997         493\n",
            "   1998         188\n",
            "   1999         318\n",
            "\n",
            "Query Performance:\n",
            "  Query Time: 0.0524 seconds\n",
            "  Average CPU Usage: 83.35%\n",
            "  Average Memory Usage: 10.40%\n",
            "  Throughput: 2196769.87 records/second\n",
            "--------------------------------------------------\n",
            "\n",
            "[Query 3: Average Price By Engine Group]\n",
            " engine_group_start_cc  average_price\n",
            "                  6500     2350916.15\n",
            "                  6000     2230549.17\n",
            "                 99500     1184000.00\n",
            "                  3500      924417.12\n",
            "                  5000      825514.12\n",
            "\n",
            "Query Performance:\n",
            "  Query Time: 0.0896 seconds\n",
            "  Average CPU Usage: 75.00%\n",
            "  Average Memory Usage: 10.40%\n",
            "  Throughput: 1284182.32 records/second\n",
            "--------------------------------------------------\n",
            "\n",
            "[Query 4: Total Cars By Location]\n",
            "     c_location  total_cars\n",
            "          Johor       18904\n",
            "          Kedah        3245\n",
            "       Kelantan        1441\n",
            "   Kuala Lumpur       31232\n",
            "         Labuan          33\n",
            "         Melaka        1647\n",
            "Negeri Sembilan        1552\n",
            "         Pahang        1509\n",
            "         Penang        6312\n",
            "          Perak        5107\n",
            "         Perlis         143\n",
            "      Putrajaya         197\n",
            "          Sabah        3440\n",
            "        Sarawak        2268\n",
            "       Selangor       36908\n",
            "     Terengganu        1063\n",
            "\n",
            "Query Performance:\n",
            "  Query Time: 0.0743 seconds\n",
            "  Average CPU Usage: 50.00%\n",
            "  Average Memory Usage: 10.40%\n",
            "  Throughput: 1546898.19 records/second\n",
            "--------------------------------------------------\n",
            "\n",
            "[Query 5: Average Mileage By Condition]\n",
            "c_condition  average_min_mileage\n",
            "        New          3906.976744\n",
            "      Recon         21719.844845\n",
            "       Used         99596.602852\n",
            "\n",
            "Query Performance:\n",
            "  Query Time: 0.0817 seconds\n",
            "  Average CPU Usage: 25.00%\n",
            "  Average Memory Usage: 10.40%\n",
            "  Throughput: 1406942.57 records/second\n",
            "--------------------------------------------------\n",
            "\n",
            "===== MULTITHREADED PROCESSING COMPLETED =====\n",
            "Total Execution Time: 0.1004 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-44dfc7f92cbf>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['engine_group_start_cc'] = (df['c_engine'] // 500) * 500\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # First fetch all data from Supabase\n",
        "    print(\"Fetching all data from Supabase...\")\n",
        "    complete_df = fetch_all_data_from_supabase(supabase)\n",
        "\n",
        "    if complete_df is not None and not complete_df.empty:\n",
        "        print(f\"Total records processed: {len(complete_df)}\")\n",
        "\n",
        "        # Run multithreaded processing on the pre-fetched data\n",
        "        run_multithreaded(complete_df)\n",
        "    else:\n",
        "        print(\"Error: Unable to fetch data from Supabase. Please check your connection and credentials.\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}