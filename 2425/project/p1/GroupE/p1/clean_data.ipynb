{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PANDAS**"
      ],
      "metadata": {
        "id": "SUIy4c4pjaoE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362435e0"
      },
      "source": [
        "#### **Data Processing using Pandas library (Benchmark)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecc93529"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bb467e3"
      },
      "source": [
        "\n",
        "> Standalone function to calculate and return processing metrics.\n",
        "- Parameters:\n",
        "    - df: pandas DataFrame being processed\n",
        "    \n",
        "- Returns:\n",
        "    - A dictionary with metrics such as CPU usage, memory usage, processing time, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15666cc2"
      },
      "outputs": [],
      "source": [
        "def calculate_processing_metrics(df):\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get initial system metrics\n",
        "    initial_cpu = psutil.cpu_percent(interval=1)\n",
        "    initial_memory = psutil.virtual_memory().percent\n",
        "\n",
        "\n",
        "    duplicate_count = (df.duplicated() == 1).sum()\n",
        "\n",
        "    # Record the end time of the operation\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Get final system metrics\n",
        "    final_cpu = psutil.cpu_percent(interval=1)\n",
        "    final_memory = psutil.virtual_memory().percent\n",
        "\n",
        "    # Calculate processing time\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    # Calculate throughput (assuming rows processed are equal to the DataFrame rows)\n",
        "    throughput = len(df) / processing_time if processing_time > 0 else 0\n",
        "\n",
        "    # Return the metrics as a dictionary\n",
        "    # Return the metrics in a nicely formatted way\n",
        "    metrics = (\n",
        "        f\"Total Rows Processed: {len(df):,} records\\n\"\n",
        "        f\"Total Processing Time: {processing_time:.4f} seconds\\n\"\n",
        "        f\"Initial CPU Usage: {initial_cpu:.2f}%\\n\"\n",
        "        f\"Final CPU Usage: {final_cpu:.2f}%\\n\"\n",
        "        f\"Memory Usage: {final_memory:.2f}%\\n\"\n",
        "        f\"Throughput (Records per Second): {throughput:.2f} records/sec\"\n",
        "    )\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571f9839"
      },
      "source": [
        "> **1. Loading Data**\n",
        "\n",
        "We load the raw dataset from the `NST_News_Articles.csv` file. The dataset contains information such as the article's title, teaser, URL, and category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81925d03",
        "outputId": "1879bbd3-40a9-40ba-e315-5165f64c417d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 110,641 records\n",
            "Total Processing Time: 1.1632 seconds\n",
            "Initial CPU Usage: 62.50%\n",
            "Final CPU Usage: 5.00%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 95121.45 records/sec\n"
          ]
        }
      ],
      "source": [
        "rd = pd.read_excel(\"NST_News_Articles.xlsx\")\n",
        "\n",
        "print(calculate_processing_metrics(rd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69289479"
      },
      "source": [
        "> **2. Handle Duplicated Data**\n",
        "\n",
        "We remove any duplicate rows from the dataset to avoid redundant data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec0f4fbe",
        "outputId": "d6728df3-0ae5-4399-f310-16ed23809036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 106,473 records\n",
            "Total Processing Time: 1.1389 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.50%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 93488.10 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = rd.drop_duplicates()\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "587d41d9"
      },
      "source": [
        "> **3. Handle Missing Data**\n",
        "\n",
        "We drop rows with missing values in key columns to maintain data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5931b7c7",
        "outputId": "f464c16e-6cfb-4f31-a341-e48addb8322a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,392 records\n",
            "Total Processing Time: 1.1954 seconds\n",
            "Initial CPU Usage: 10.00%\n",
            "Final CPU Usage: 4.60%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 88163.65 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.dropna()\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOv3rm7wkVru"
      },
      "source": [
        "> **4. Clean the Teaser Column**\n",
        "\n",
        "We clean the `Teaser` column by removing unwanted characters and ensuring that the teaser follows a standard format (e.g., extracting place and content from the teaser)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143e98ba",
        "outputId": "42c9a37b-0f92-4ebe-cf3f-452fb9acf25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,392 records\n",
            "Total Processing Time: 1.1841 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.50%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 89002.29 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned['Teaser'] = df_cleaned['Teaser'].str.replace(r'[^a-zA-Z0-9: ,]', '', regex=True)\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0130689",
        "outputId": "206e5a7a-19dd-434b-d22e-4710fd19a812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.1749 seconds\n",
            "Initial CPU Usage: 5.60%\n",
            "Final CPU Usage: 27.80%\n",
            "Memory Usage: 21.40%\n",
            "Throughput (Records per Second): 87724.97 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned[df_cleaned['Teaser'].str.contains(':')]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e761b4ff"
      },
      "source": [
        "> **5. Splitting the place from 'Teaser' column**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dff9f438",
        "outputId": "84c4998b-2598-4a20-f021-eb9d284442d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.2900 seconds\n",
            "Initial CPU Usage: 57.80%\n",
            "Final CPU Usage: 33.30%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 79902.15 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned[['Place', 'Teaser']] = df_cleaned['Teaser'].str.split(':', n=1, expand=True)\n",
        "print(calculate_processing_metrics(df_cleaned))\n",
        "\n",
        "#KUALA LUMPUR: The Central Database Hub (PADU) system has recorded a total of 2.38 million individual information updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqqBeh1wk9wN"
      },
      "source": [
        "> **6. Extract and Standardize Place Names**\n",
        "\n",
        "We standardize the place names, convert them to uppercase, and remove any country names or other non-relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ccb0f67",
        "outputId": "5b9b7114-e331-4637-f0a7-2b3cc8592250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.1932 seconds\n",
            "Initial CPU Usage: 4.50%\n",
            "Final CPU Usage: 5.50%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 86382.48 records/sec\n"
          ]
        }
      ],
      "source": [
        "place_corrections = {\n",
        "    'ALOR STAR': 'ALOR SETAR', 'AOR SETAR': 'ALOR SETAR','LOR STAR':'ALOR SETAR','ASTANA KAZAKHSTAN':'ASTANA',\n",
        "    'BALIK PULAI':'BALIK PULAU','BATANG AI': 'BATANG KALI', 'BAGAN DATOH':'BAGAN DATUK',\n",
        "    'CAMERON HIGHLAND': 'CAMERON HIGHLANDS','CHIANGMAI': 'CHIANG MAI','COLOMBO SRI LANKA': 'COLOMBO',\n",
        "    'FRANK': 'FRANKFURT',\n",
        "    'GUAMUSANG': 'GUA MUSANG','GUA MUSANG POS SIMPOR': 'GUA MUSANG',\n",
        "    'DANANG': 'DA NANG',\n",
        "    'GEOGE TOWN': 'GEORGE TOWN','GEORGETOWN': 'GEORGE TOWN','JERTIH':'JERTEH',\n",
        "    'JOHOR BARU': 'JOHOR BAHRU', 'JOHOR BAHU': 'JOHOR BAHRU','JOHOR BHARU': 'JOHOR BAHRU','JOHOR BARY': 'JOHOR BAHRU','JOHOR BAHARU': 'JOHOR BAHRU',\n",
        "    'JOHOR BARU KUALA LUMPUR':'JOHOR BAHRU','JOHOR BARUSINGAPORE':'JOHOR BAHRU',\n",
        "    'KUALA KUBU BARU':'KUALA KUBU BAHRU','KUALA KUBU BAHARU':'KUALA KUBU BAHRU',\n",
        "    'UALA LUMPUR': 'KUALA LUMPUR','KUALKUALA LUMPUR':'KUALA LUMPUR','SEPT  KUALA LUMPUR': 'KUALA LUMPR','KKUALA LUMUR': 'KUALA LUMPUR',\n",
        "    'KIALA LUMUPUR': 'KUALA LUMPUR', 'IKUALA LUMPUR': 'KUALA LUMPUR','KUALAA LUMPUR':'KUALA LUMPUR','KUALALUMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMUR':'KUALA LUMPUR','KUALA LUMPU':'KUALA LUMPUR','KUALA LUMPURHONG KONG':'KUALA LUMPUR','KUALA LIMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMPURJAKARTA':'KUALA LUMPUR','KUALA KUMPUR':'KUALA LUMPUR','KUALA NERUS TERENGGANU':'KUALA NERUS',\n",
        "    'KUALATERENGGANU':'KUALA TERENGGANU','KUALA TERENGANU':'KUALA TERENGGANU','KUALA TERENGAGNU':'KUALA TERENGGANU','KUALA TENGGANU':'KUALA TERENGGANU',\n",
        "    'KULA LUMPUR':'KUALA LUMPUR','KUCHINGL':'KUCHING','KUANG':'KLUANG',\n",
        "    'KUAL LUMPUR':'KUALA LUMPUR','KUALA  LUMPUR':'KUALA LUMPUR',\n",
        "    'UALA TERENGGANU': 'KUALA TERENGGANU','KKOTA KINABALU':'KOTA KINABALU','KOTA KINABAU':'KOTA KINABALU','KOTA KINBALU':'KOTA KINABALU','KOTA  KINABALU':'KOTA KINABALU',\n",
        "    'KOTA  BARU':'KOTA BAHRU', 'KOTA BAHARU':'KOTA BAHRU','KOTA BARU':'KOTA BAHRU','KOTA BARUGEORGE TOWN':'KOTA BAHRU',\n",
        "    'LABUAN BAJO INDONESIA':'LABUAN BAJO','LONDONKUALA LUMPUR':'LONDON','LONDON TUES':'LONDON','LENGONG':'LENGGONG','LAMGKAWI':'LANGKAWI',\n",
        "    'MARNG':'MARANG','MELAKA': 'MALACCA','MEKALA':'MALACCA','MANAMA BAHRAIN': 'MANAMA',\n",
        "    'NIBONG TEBA':'NIBONG TEBAL','NEW DELHI INDIA':'NEW DELHI','NEW DELH':'NEW DELHI','NARATHIWAT SOUTHERN THAILAND':'NARATHIWAT','MUNDOK SOUTHERN THAILAND':'MUNDOK',\n",
        "    'PARISBEIJING':'PARIS',\n",
        "    'PUTRAJAYAS': 'PUTRAJAYA','PUTRAYAJA': 'PUTRAJAYA','PUTRJAYA': 'PUTRAJAYA','PPUTRAJAYA': 'PUTRAJAYA','PATTANI THAILAND':'PATTANI','PASIR PUTIH':'PASIR PUTEH',\n",
        "    'PORT MORESBY PAPUA NEW GUINEA': 'PORT MORESBY','PANGKOR ISLAND':'PANGKOR','PULAU PERHENTIAN KECIL TERENGGANU':'PULAU PERHENTIAN',\n",
        "    'SEBERANG PERAI': 'SEBERANG PRAI','SUNNYLANDS CALIFORNIA': 'SUNNYLANDS','SUNGAI GOLOK THAILAND':'SUNGAI GOLOK',\n",
        "    'SUBANG': 'SUBANG JAYA','SONGKLA': 'SONGKHLA','SHAH  ALAM': 'SHAH ALAM','SEMENYEH': 'SEMENYIH','SELANGAU': 'SELANGOR','SARI': 'SARIKEI',\n",
        "    'SAMARAHAN': 'SAMARKAND','SADAO THAILAND': 'SADAO',   'ALSHAH ALAM': 'SHAH ALAM',\n",
        "    'THE HAGUE NETHERLANDS': 'THE HAGUE','TASHKENTL': 'TASHKENT','TAKBAI SOUTHERN THAILAND': 'TAKBAI','TAK': 'TAK THAILAND',\n",
        "    'VALLETTA MALTA': 'VALLETTA','VIENTIANE LAOS': 'VIENTIANE','VLADIVOSTOK RUSSIA': 'VLADIVOSTOK','VALETTA':'VALLETTA',\n",
        "    'ULAANBAATAR  MONGOLIA': 'ULAANBAATAR','ULAANBAATAR MONGOLIA': 'ULAANBAATAR','ULAANBAATAAR': 'ULAANBAATAR',\n",
        "    'WASHINGTON DC': 'WASHINGTON',\n",
        "\n",
        "}\n",
        "\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.upper()\n",
        "df_cleaned['Place'] = df_cleaned['Place'].replace(place_corrections)\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.split(',').str[0]\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.replace(r'[^a-zA-Z\\s]+', '', regex=True)\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cce9b82d",
        "outputId": "bb66ae20-e0ed-4cef-eeeb-f8c6da88908d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,768 records\n",
            "Total Processing Time: 1.1880 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.00%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 86501.67 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Count the number of articles per city\n",
        "city_counts = df_cleaned['Place'].value_counts()\n",
        "\n",
        "# Set a threshold: keep only cities with at least N articles (e.g., N=2)\n",
        "threshold = 2\n",
        "valid_cities = city_counts[city_counts >= threshold].index\n",
        "\n",
        "# Save the valid cities to a CSV file\n",
        "pd.Series(valid_cities).to_csv('valid_cities.csv', index=False, header=False)\n",
        "\n",
        "# Filter the DataFrame to keep only valid cities\n",
        "df_cleaned = df_cleaned[df_cleaned['Place'].isin(valid_cities)]\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pw7tjuNupRa",
        "outputId": "46cddc14-23d0-40d5-fef9-4365b7318d09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,768 records\n",
            "Total Processing Time: 1.1859 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.50%\n",
            "Memory Usage: 21.50%\n",
            "Throughput (Records per Second): 86658.99 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Filter the DataFrame to keep only rows where 'Place' is in valid_cities\n",
        "df_cleaned = df_cleaned[df_cleaned['Place'].isin(valid_cities)]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d968cb1"
      },
      "source": [
        "> **7. Extract Date from URL**\n",
        "\n",
        "We extract the date in `YYYY/MM` format from the URL and add it as a separate column in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5d715eb",
        "outputId": "3cba48a4-c85b-4ff3-fc65-c0bd4cc4fc80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date column extracted from the URL.\n",
            "Total Rows Processed: 102,768 records\n",
            "Total Processing Time: 1.2172 seconds\n",
            "Initial CPU Usage: 5.50%\n",
            "Final CPU Usage: 57.40%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 84433.14 records/sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_cleaned['Date'] = df_cleaned['URL'].str.extract(r'(\\d{4}/\\d{2})')\n",
        "print(\"Date column extracted from the URL.\")\n",
        "print(calculate_processing_metrics(df_cleaned))  # Metrics after extracting the Date column\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67f359f8"
      },
      "source": [
        "> **8. Final Dataset**\n",
        "\n",
        "After cleaning and transforming the data, we earrange dataframe and export the cleaned dataset to a new CSV file (`finalData.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b3fb2f6",
        "outputId": "147d08d5-6b26-443c-cd18-0d29a4e070ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,768 records\n",
            "Total Processing Time: 1.2065 seconds\n",
            "Initial CPU Usage: 60.60%\n",
            "Final CPU Usage: 14.60%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 85178.44 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Rearrange the columns to the desired order\n",
        "df_cleaned = df_cleaned[['Place', 'Date', 'Category', 'Title','Teaser']]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4ae4737"
      },
      "outputs": [],
      "source": [
        "sorted_df = df_cleaned.sort_values(by=['Place'])\n",
        "sorted_df.to_csv('finalData.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b702fdfa"
      },
      "source": []
    }
  ]
}