{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **POLARS**"
      ],
      "metadata": {
        "id": "EyVeE3nGjq1-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sL7TnLcjwlD"
      },
      "source": [
        "#### **Data Processing using Polars library (Benchmark)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastexcel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaEBRn11UES_",
        "outputId": "5fdb7986-7799-4e28-d0a4-c77b8327a584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastexcel in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from fastexcel) (18.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "possq3WPjwlG"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import re\n",
        "import time\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0I9CBeojwlG"
      },
      "source": [
        "\n",
        "> Standalone function to calculate and return processing metrics.\n",
        "- Parameters:\n",
        "    - df: Polars DataFrame being processed\n",
        "    \n",
        "- Returns:\n",
        "    - A dictionary with metrics such as CPU usage, memory usage, processing time, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMhZIsF5jwlH"
      },
      "outputs": [],
      "source": [
        "def calculate_processing_metrics(df):\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get initial system metrics\n",
        "    initial_cpu = psutil.cpu_percent(interval=1)\n",
        "    initial_memory = psutil.virtual_memory().percent\n",
        "\n",
        "\n",
        "    # Get row count (handle both Polars and Pandas/Dask DataFrames)\n",
        "    if isinstance(df, pl.DataFrame):\n",
        "        row_count = df.height\n",
        "    else:\n",
        "        row_count = len(df)\n",
        "\n",
        "    # Record the end time of the operation\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Get final system metrics\n",
        "    final_cpu = psutil.cpu_percent(interval=1)\n",
        "    final_memory = psutil.virtual_memory().percent\n",
        "\n",
        "    # Calculate processing time\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    # Calculate throughput (assuming rows processed are equal to the DataFrame rows)\n",
        "    throughput = row_count / processing_time if processing_time > 0 else 0\n",
        "\n",
        "    # Return the metrics as a dictionary\n",
        "    # Return the metrics in a nicely formatted way\n",
        "    metrics = (\n",
        "        f\"Total Rows Processed: {row_count:,} records\\n\"\n",
        "        f\"Total Processing Time: {processing_time:.4f} seconds\\n\"\n",
        "        f\"Initial CPU Usage: {initial_cpu:.2f}%\\n\"\n",
        "        f\"Final CPU Usage: {final_cpu:.2f}%\\n\"\n",
        "        f\"Memory Usage: {final_memory:.2f}%\\n\"\n",
        "        f\"Throughput (Records per Second): {throughput:.2f} records/sec\"\n",
        "    )\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAezzdqJjwlH"
      },
      "source": [
        "> **1. Loading Data**\n",
        "\n",
        "We load the raw dataset from the `NST_News_Articles.csv` file. The dataset contains information such as the article's title, teaser, URL, and category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2961c214-1c56-4ad5-ea52-18748ae82999",
        "id": "_buG7OnCjwlH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 110,641 records\n",
            "Total Processing Time: 1.0011 seconds\n",
            "Initial CPU Usage: 61.00%\n",
            "Final CPU Usage: 25.90%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 110522.87 records/sec\n"
          ]
        }
      ],
      "source": [
        "rd = pl.read_excel(\"NST_News_Articles.xlsx\")\n",
        "\n",
        "print(calculate_processing_metrics(rd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcSjn2JJjwlI"
      },
      "source": [
        "> **2. Handle Duplicated Data**\n",
        "\n",
        "We remove any duplicate rows from the dataset to avoid redundant data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49d81eb-1222-4072-ae42-439f6926629f",
        "id": "SZeVFZqMjwlI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 106,473 records\n",
            "Total Processing Time: 1.0006 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.10%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 106406.18 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = rd.unique()\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv850ERjjwlI"
      },
      "source": [
        "> **3. Handle Missing Data**\n",
        "\n",
        "We drop rows with missing values in key columns to maintain data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbf21aa-a9e3-4295-c88f-12baf4be3690",
        "id": "egOrAxDYjwlJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,392 records\n",
            "Total Processing Time: 1.0005 seconds\n",
            "Initial CPU Usage: 5.00%\n",
            "Final CPU Usage: 9.00%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 105340.71 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.drop_nulls()\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **4. Clean the Teaser Column**\n",
        "\n",
        "We clean the `Teaser` column by removing unwanted characters and ensuring that the teaser follows a standard format (e.g., extracting place and content from the teaser)."
      ],
      "metadata": {
        "id": "81aym6mnjwlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f1e11c-ea4a-4ade-e3eb-77428786790a",
        "id": "UejB6qGwjwlK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,392 records\n",
            "Total Processing Time: 1.0006 seconds\n",
            "Initial CPU Usage: 4.50%\n",
            "Final CPU Usage: 5.10%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 105331.90 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.with_columns(\n",
        "    pl.col('Teaser').str.replace_all(r'[^a-zA-Z0-9: ,]', '')\n",
        ")\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c36093-6608-47e9-9723-7901ece43abd",
        "id": "kzXPfckNjwlK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0006 seconds\n",
            "Initial CPU Usage: 5.50%\n",
            "Final CPU Usage: 5.60%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 103003.91 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.filter(pl.col('Teaser').str.contains(':'))\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EZACPGMjwlK"
      },
      "source": [
        "> **5. Splitting the place from 'Teaser' column**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72452fd4-672d-44e7-b543-cb53f2b6de3b",
        "id": "c_y735hajwlK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0005 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 51.30%\n",
            "Memory Usage: 21.60%\n",
            "Throughput (Records per Second): 103016.97 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Split Teaser into Place and Teaser - corrected version\n",
        "df_cleaned = df_cleaned.with_columns([\n",
        "    pl.col('Teaser').str.split(':').list.get(0).alias('Place'),\n",
        "    pl.col('Teaser').str.split(':').list.get(1).alias('Teaser_New')\n",
        "])\n",
        "\n",
        "# Replace the old Teaser with the new one\n",
        "df_cleaned = df_cleaned.with_columns([\n",
        "    pl.col('Teaser_New').alias('Teaser')\n",
        "]).drop('Teaser_New')\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n",
        "\n",
        "#KUALA LUMPUR: The Central Database Hub (PADU) system has recorded a total of 2.38 million individual information updates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **6. Extract and Standardize Place Names**\n",
        "\n",
        "We standardize the place names, convert them to uppercase, and remove any country names or other non-relevant information."
      ],
      "metadata": {
        "id": "twcDLR1zjwlL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36dbbde-719d-4f80-fb90-6662fbcd7db8",
        "id": "nkgZLPe8jwlL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0008 seconds\n",
            "Initial CPU Usage: 14.80%\n",
            "Final CPU Usage: 4.00%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 102986.27 records/sec\n"
          ]
        }
      ],
      "source": [
        "place_corrections = {\n",
        "    'ALOR STAR': 'ALOR SETAR', 'AOR SETAR': 'ALOR SETAR','LOR STAR':'ALOR SETAR','ASTANA KAZAKHSTAN':'ASTANA',\n",
        "    'BALIK PULAI':'BALIK PULAU','BATANG AI': 'BATANG KALI', 'BAGAN DATOH':'BAGAN DATUK',\n",
        "    'CAMERON HIGHLAND': 'CAMERON HIGHLANDS','CHIANGMAI': 'CHIANG MAI','COLOMBO SRI LANKA': 'COLOMBO',\n",
        "    'FRANK': 'FRANKFURT',\n",
        "    'GUAMUSANG': 'GUA MUSANG','GUA MUSANG POS SIMPOR': 'GUA MUSANG',\n",
        "    'DANANG': 'DA NANG',\n",
        "    'GEOGE TOWN': 'GEORGE TOWN','GEORGETOWN': 'GEORGE TOWN','JERTIH':'JERTEH',\n",
        "    'JOHOR BARU': 'JOHOR BAHRU', 'JOHOR BAHU': 'JOHOR BAHRU','JOHOR BHARU': 'JOHOR BAHRU','JOHOR BARY': 'JOHOR BAHRU','JOHOR BAHARU': 'JOHOR BAHRU',\n",
        "    'JOHOR BARU KUALA LUMPUR':'JOHOR BAHRU','JOHOR BARUSINGAPORE':'JOHOR BAHRU',\n",
        "    'KUALA KUBU BARU':'KUALA KUBU BAHRU','KUALA KUBU BAHARU':'KUALA KUBU BAHRU',\n",
        "    'UALA LUMPUR': 'KUALA LUMPUR','KUALKUALA LUMPUR':'KUALA LUMPUR','SEPT  KUALA LUMPUR': 'KUALA LUMPR','KKUALA LUMUR': 'KUALA LUMPUR',\n",
        "    'KIALA LUMUPUR': 'KUALA LUMPUR', 'IKUALA LUMPUR': 'KUALA LUMPUR','KUALAA LUMPUR':'KUALA LUMPUR','KUALALUMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMUR':'KUALA LUMPUR','KUALA LUMPU':'KUALA LUMPUR','KUALA LUMPURHONG KONG':'KUALA LUMPUR','KUALA LIMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMPURJAKARTA':'KUALA LUMPUR','KUALA KUMPUR':'KUALA LUMPUR','KUALA NERUS TERENGGANU':'KUALA NERUS',\n",
        "    'KUALATERENGGANU':'KUALA TERENGGANU','KUALA TERENGANU':'KUALA TERENGGANU','KUALA TERENGAGNU':'KUALA TERENGGANU','KUALA TENGGANU':'KUALA TERENGGANU',\n",
        "    'KULA LUMPUR':'KUALA LUMPUR','KUCHINGL':'KUCHING','KUANG':'KLUANG',\n",
        "    'KUAL LUMPUR':'KUALA LUMPUR','KUALA  LUMPUR':'KUALA LUMPUR',\n",
        "    'UALA TERENGGANU': 'KUALA TERENGGANU','KKOTA KINABALU':'KOTA KINABALU','KOTA KINABAU':'KOTA KINABALU','KOTA KINBALU':'KOTA KINABALU','KOTA  KINABALU':'KOTA KINABALU',\n",
        "    'KOTA  BARU':'KOTA BAHRU', 'KOTA BAHARU':'KOTA BAHRU','KOTA BARU':'KOTA BAHRU','KOTA BARUGEORGE TOWN':'KOTA BAHRU',\n",
        "    'LABUAN BAJO INDONESIA':'LABUAN BAJO','LONDONKUALA LUMPUR':'LONDON','LONDON TUES':'LONDON','LENGONG':'LENGGONG','LAMGKAWI':'LANGKAWI',\n",
        "    'MARNG':'MARANG','MELAKA': 'MALACCA','MEKALA':'MALACCA','MANAMA BAHRAIN': 'MANAMA',\n",
        "    'NIBONG TEBA':'NIBONG TEBAL','NEW DELHI INDIA':'NEW DELHI','NEW DELH':'NEW DELHI','NARATHIWAT SOUTHERN THAILAND':'NARATHIWAT','MUNDOK SOUTHERN THAILAND':'MUNDOK',\n",
        "    'PARISBEIJING':'PARIS',\n",
        "    'PUTRAJAYAS': 'PUTRAJAYA','PUTRAYAJA': 'PUTRAJAYA','PUTRJAYA': 'PUTRAJAYA','PPUTRAJAYA': 'PUTRAJAYA','PATTANI THAILAND':'PATTANI','PASIR PUTIH':'PASIR PUTEH',\n",
        "    'PORT MORESBY PAPUA NEW GUINEA': 'PORT MORESBY','PANGKOR ISLAND':'PANGKOR','PULAU PERHENTIAN KECIL TERENGGANU':'PULAU PERHENTIAN',\n",
        "    'SEBERANG PERAI': 'SEBERANG PRAI','SUNNYLANDS CALIFORNIA': 'SUNNYLANDS','SUNGAI GOLOK THAILAND':'SUNGAI GOLOK',\n",
        "    'SUBANG': 'SUBANG JAYA','SONGKLA': 'SONGKHLA','SHAH  ALAM': 'SHAH ALAM','SEMENYEH': 'SEMENYIH','SELANGAU': 'SELANGOR','SARI': 'SARIKEI',\n",
        "    'SAMARAHAN': 'SAMARKAND','SADAO THAILAND': 'SADAO',   'ALSHAH ALAM': 'SHAH ALAM',\n",
        "    'THE HAGUE NETHERLANDS': 'THE HAGUE','TASHKENTL': 'TASHKENT','TAKBAI SOUTHERN THAILAND': 'TAKBAI','TAK': 'TAK THAILAND',\n",
        "    'VALLETTA MALTA': 'VALLETTA','VIENTIANE LAOS': 'VIENTIANE','VLADIVOSTOK RUSSIA': 'VLADIVOSTOK','VALETTA':'VALLETTA',\n",
        "    'ULAANBAATAR  MONGOLIA': 'ULAANBAATAR','ULAANBAATAR MONGOLIA': 'ULAANBAATAR','ULAANBAATAAR': 'ULAANBAATAR',\n",
        "    'WASHINGTON DC': 'WASHINGTON', 'KKUALA LUMPURR': 'KUALA LUMPUR',\n",
        "\n",
        "}\n",
        "\n",
        "df_cleaned = df_cleaned.with_columns(\n",
        "    pl.col('Place').str.to_uppercase()\n",
        ")\n",
        "for old, new in place_corrections.items():\n",
        "    df_cleaned = df_cleaned.with_columns(\n",
        "        pl.col('Place').str.replace_all(old, new)\n",
        "    )\n",
        "df_cleaned = df_cleaned.with_columns(\n",
        "    pl.col('Place').str.split(',').list.first()\n",
        ")\n",
        "df_cleaned = df_cleaned.with_columns(\n",
        "    pl.col('Place').str.replace_all(r'[^a-zA-Z\\s]+', '')\n",
        ")\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of articles per city\n",
        "city_counts = (\n",
        "    df_cleaned\n",
        "    .group_by(\"Place\")\n",
        "    .agg(pl.len().alias(\"count\"))\n",
        "    .filter(pl.col(\"count\") >= 2)\n",
        ")\n",
        "\n",
        "# Extract valid cities\n",
        "valid_cities = city_counts[\"Place\"]\n",
        "\n",
        "# Save the valid cities to a CSV file\n",
        "pl.DataFrame({\"Place\": valid_cities}).write_csv(\"valid_cities.csv\")\n",
        "\n",
        "# Filter the original DataFrame\n",
        "df_cleaned = df_cleaned.filter(pl.col(\"Place\").is_in(valid_cities))\n",
        "\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eMHXooxUqZs",
        "outputId": "a70e1834-680c-49fb-eeb8-766b45344665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0006 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 4.50%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 102697.99 records/sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to keep only rows where 'Place' is in valid_cities\n",
        "df_cleaned = df_cleaned.filter(pl.col(\"Place\").is_in(valid_cities))\n",
        "\n",
        "# If your calculate_processing_metrics function is pandas-based, convert to pandas\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHlKgr6QW_wF",
        "outputId": "1e1ff26f-3686-4b7c-e66a-a7aedc9fd276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0005 seconds\n",
            "Initial CPU Usage: 5.10%\n",
            "Final CPU Usage: 5.00%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 102705.31 records/sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCkBjtJ8jwlM"
      },
      "source": [
        "> **7. Extract Date from URL**\n",
        "\n",
        "We extract the date in `YYYY/MM` format from the URL and add it as a separate column in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b81dd4c-d275-484c-f09c-fd89a1e378ab",
        "id": "YtT71ZHmjwlN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date column extracted from the URL.\n",
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0008 seconds\n",
            "Initial CPU Usage: 5.60%\n",
            "Final CPU Usage: 4.50%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 102675.12 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.with_columns(\n",
        "    pl.col(\"URL\").str.extract(r\"(\\d{4}/\\d{2})\").alias(\"Date\")\n",
        ")\n",
        "\n",
        "print(\"Date column extracted from the URL.\")\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ73TJwxjwlN"
      },
      "source": [
        "> **8. Final Dataset**\n",
        "\n",
        "After cleaning and transforming the data, we earrange dataframe and export the cleaned dataset to a new CSV file (`finalData.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08d9ee8-f954-4777-9541-522b646402bd",
        "id": "ZUsn_evqjwlN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0015 seconds\n",
            "Initial CPU Usage: 6.50%\n",
            "Final CPU Usage: 5.50%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 102605.21 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Rearrange the columns to the desired order\n",
        "df_cleaned = df_cleaned[['Place', 'Date', 'Category', 'Title','Teaser']]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jZvzt1coRVVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQaQHLt-jwlN"
      },
      "outputs": [],
      "source": [
        "sorted_df = df_cleaned.sort(\"Place\")\n",
        "\n",
        "sorted_df.write_csv('finalData.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWuPdKk1jwlO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODIN**"
      ],
      "metadata": {
        "id": "cY1PzbaWjghL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svlkzv1pjoBa"
      },
      "source": [
        "#### **Data Processing using Modin library (Benchmark)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modin[ray] -q\n"
      ],
      "metadata": {
        "id": "xwskjnKiaDHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaIt6_T5joBb"
      },
      "outputs": [],
      "source": [
        "import modin.pandas as pd\n",
        "import re\n",
        "import time\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zckn1S8joBb"
      },
      "source": [
        "\n",
        "> Standalone function to calculate and return processing metrics.\n",
        "- Parameters:\n",
        "    - df: modin pandas DataFrame being processed\n",
        "    \n",
        "- Returns:\n",
        "    - A dictionary with metrics such as CPU usage, memory usage, processing time, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiT637kPjoBb"
      },
      "outputs": [],
      "source": [
        "def calculate_processing_metrics(df):\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get initial system metrics\n",
        "    initial_cpu = psutil.cpu_percent(interval=1)\n",
        "    initial_memory = psutil.virtual_memory().percent\n",
        "\n",
        "    row_count = len(df)\n",
        "\n",
        "    # Record the end time of the operation\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Get final system metrics\n",
        "    final_cpu = psutil.cpu_percent(interval=1)\n",
        "    final_memory = psutil.virtual_memory().percent\n",
        "\n",
        "    # Calculate processing time\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    # Calculate throughput (assuming rows processed are equal to the DataFrame rows)\n",
        "    throughput = row_count / processing_time if processing_time > 0 else 0\n",
        "\n",
        "    # Return the metrics as a dictionary\n",
        "    # Return the metrics in a nicely formatted way\n",
        "    metrics = (\n",
        "        f\"Total Rows Processed: {row_count:,} records\\n\"\n",
        "        f\"Total Processing Time: {processing_time:.4f} seconds\\n\"\n",
        "        f\"Initial CPU Usage: {initial_cpu:.2f}%\\n\"\n",
        "        f\"Final CPU Usage: {final_cpu:.2f}%\\n\"\n",
        "        f\"Memory Usage: {final_memory:.2f}%\\n\"\n",
        "        f\"Throughput (Records per Second): {throughput:.2f} records/sec\"\n",
        "    )\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sonCZZrNjoBc"
      },
      "source": [
        "> **1. Loading Data**\n",
        "\n",
        "We load the raw dataset from the `NST_News_Articles.csv` file. The dataset contains information such as the article's title, teaser, URL, and category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5826e848-6afa-4a53-d604-5765887b9adf",
        "id": "YWaikq_DjoBc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: Parallel `read_excel` is a new feature! If you run into any problems, please visit https://github.com/modin-project/modin/issues. If you find a new issue and can't file it on GitHub, please email bug_reports@modin.org.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 110,641 records\n",
            "Total Processing Time: 1.0011 seconds\n",
            "Initial CPU Usage: 56.60%\n",
            "Final CPU Usage: 54.60%\n",
            "Memory Usage: 22.30%\n",
            "Throughput (Records per Second): 110520.42 records/sec\n"
          ]
        }
      ],
      "source": [
        "rd = pd.read_excel(\"NST_News_Articles.xlsx\")\n",
        "\n",
        "print(calculate_processing_metrics(rd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzpO2uoOjoBc"
      },
      "source": [
        "> **2. Handle Duplicated Data**\n",
        "\n",
        "We remove any duplicate rows from the dataset to avoid redundant data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80c8485-4a23-4d55-bc50-41ae5ffb0d26",
        "id": "IuqQUeeZjoBd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 106,473 records\n",
            "Total Processing Time: 1.0742 seconds\n",
            "Initial CPU Usage: 50.00%\n",
            "Final CPU Usage: 6.00%\n",
            "Memory Usage: 22.40%\n",
            "Throughput (Records per Second): 99120.34 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = rd.drop_duplicates()\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNSARjzYjoBd"
      },
      "source": [
        "> **3. Handle Missing Data**\n",
        "\n",
        "We drop rows with missing values in key columns to maintain data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3f6505-da25-4bcd-f128-13c11513b8e4",
        "id": "tuYyFW_ijoBd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,393 records\n",
            "Total Processing Time: 1.0721 seconds\n",
            "Initial CPU Usage: 25.10%\n",
            "Final CPU Usage: 4.00%\n",
            "Memory Usage: 22.40%\n",
            "Throughput (Records per Second): 98303.39 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned.dropna()\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **4. Clean the Teaser Column**\n",
        "\n",
        "We clean the `Teaser` column by removing unwanted characters and ensuring that the teaser follows a standard format (e.g., extracting place and content from the teaser)."
      ],
      "metadata": {
        "id": "xdgQPNMBjoBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d497d8-928c-41ab-c044-fc6d39635555",
        "id": "Jo8Uux1VjoBe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 105,393 records\n",
            "Total Processing Time: 1.0007 seconds\n",
            "Initial CPU Usage: 48.00%\n",
            "Final CPU Usage: 5.00%\n",
            "Memory Usage: 22.40%\n",
            "Throughput (Records per Second): 105318.57 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned['Teaser'] = df_cleaned['Teaser'].astype(str).str.replace(r'[^a-zA-Z0-9: ,]', '', regex=True)\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7766e2-f0e3-4c47-c406-dbb5c0efee76",
        "id": "NbGRKvArjoBe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0449 seconds\n",
            "Initial CPU Usage: 46.80%\n",
            "Final CPU Usage: 4.10%\n",
            "Memory Usage: 22.40%\n",
            "Throughput (Records per Second): 98638.17 records/sec\n"
          ]
        }
      ],
      "source": [
        "df_cleaned = df_cleaned[df_cleaned['Teaser'].str.contains(':')]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT-NdcdGjoBf"
      },
      "source": [
        "> **5. Splitting the place from 'Teaser' column**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8928c8fb-e652-44c1-8b7a-777b66398621",
        "id": "mXONHrfTjoBf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: <function Series.split> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
            "UserWarning: `DataFrame.setitem_unhashable_key` is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0007 seconds\n",
            "Initial CPU Usage: 19.80%\n",
            "Final CPU Usage: 55.30%\n",
            "Memory Usage: 22.30%\n",
            "Throughput (Records per Second): 102998.93 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Split \"Teaser\" into \"Place\" and \"Teaser_New\"\n",
        "df_cleaned[['Place', 'Teaser']] = df_cleaned['Teaser'].str.split(':', n=1, expand=True)\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n",
        "\n",
        "#KUALA LUMPUR: The Central Database Hub (PADU) system has recorded a total of 2.38 million individual information updates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **6. Extract and Standardize Place Names**\n",
        "\n",
        "We standardize the place names, convert them to uppercase, and remove any country names or other non-relevant information."
      ],
      "metadata": {
        "id": "Y5BAN9jxjoBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393171ab-6efe-4ecb-9fee-c8a762d82ced",
        "id": "5Y4Jo0ByjoBg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 103,070 records\n",
            "Total Processing Time: 1.0008 seconds\n",
            "Initial CPU Usage: 100.00%\n",
            "Final CPU Usage: 100.00%\n",
            "Memory Usage: 22.40%\n",
            "Throughput (Records per Second): 102990.74 records/sec\n"
          ]
        }
      ],
      "source": [
        "place_corrections = {\n",
        "    'ALOR STAR': 'ALOR SETAR', 'AOR SETAR': 'ALOR SETAR','LOR STAR':'ALOR SETAR','ASTANA KAZAKHSTAN':'ASTANA',\n",
        "    'BALIK PULAI':'BALIK PULAU','BATANG AI': 'BATANG KALI', 'BAGAN DATOH':'BAGAN DATUK',\n",
        "    'CAMERON HIGHLAND': 'CAMERON HIGHLANDS','CHIANGMAI': 'CHIANG MAI','COLOMBO SRI LANKA': 'COLOMBO',\n",
        "    'FRANK': 'FRANKFURT',\n",
        "    'GUAMUSANG': 'GUA MUSANG','GUA MUSANG POS SIMPOR': 'GUA MUSANG',\n",
        "    'DANANG': 'DA NANG',\n",
        "    'GEOGE TOWN': 'GEORGE TOWN','GEORGETOWN': 'GEORGE TOWN','JERTIH':'JERTEH',\n",
        "    'JOHOR BARU': 'JOHOR BAHRU', 'JOHOR BAHU': 'JOHOR BAHRU','JOHOR BHARU': 'JOHOR BAHRU','JOHOR BARY': 'JOHOR BAHRU','JOHOR BAHARU': 'JOHOR BAHRU',\n",
        "    'JOHOR BARU KUALA LUMPUR':'JOHOR BAHRU','JOHOR BARUSINGAPORE':'JOHOR BAHRU',\n",
        "    'KUALA KUBU BARU':'KUALA KUBU BAHRU','KUALA KUBU BAHARU':'KUALA KUBU BAHRU',\n",
        "    'UALA LUMPUR': 'KUALA LUMPUR','KUALKUALA LUMPUR':'KUALA LUMPUR','SEPT  KUALA LUMPUR': 'KUALA LUMPR','KKUALA LUMUR': 'KUALA LUMPUR',\n",
        "    'KIALA LUMUPUR': 'KUALA LUMPUR', 'IKUALA LUMPUR': 'KUALA LUMPUR','KUALAA LUMPUR':'KUALA LUMPUR','KUALALUMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMUR':'KUALA LUMPUR','KUALA LUMPU':'KUALA LUMPUR','KUALA LUMPURHONG KONG':'KUALA LUMPUR','KUALA LIMPUR':'KUALA LUMPUR',\n",
        "    'KUALA LUMPURJAKARTA':'KUALA LUMPUR','KUALA KUMPUR':'KUALA LUMPUR','KUALA NERUS TERENGGANU':'KUALA NERUS',\n",
        "    'KUALATERENGGANU':'KUALA TERENGGANU','KUALA TERENGANU':'KUALA TERENGGANU','KUALA TERENGAGNU':'KUALA TERENGGANU','KUALA TENGGANU':'KUALA TERENGGANU',\n",
        "    'KULA LUMPUR':'KUALA LUMPUR','KUCHINGL':'KUCHING','KUANG':'KLUANG',\n",
        "    'KUAL LUMPUR':'KUALA LUMPUR','KUALA  LUMPUR':'KUALA LUMPUR',\n",
        "    'UALA TERENGGANU': 'KUALA TERENGGANU','KKOTA KINABALU':'KOTA KINABALU','KOTA KINABAU':'KOTA KINABALU','KOTA KINBALU':'KOTA KINABALU','KOTA  KINABALU':'KOTA KINABALU',\n",
        "    'KOTA  BARU':'KOTA BAHRU', 'KOTA BAHARU':'KOTA BAHRU','KOTA BARU':'KOTA BAHRU','KOTA BARUGEORGE TOWN':'KOTA BAHRU',\n",
        "    'LABUAN BAJO INDONESIA':'LABUAN BAJO','LONDONKUALA LUMPUR':'LONDON','LONDON TUES':'LONDON','LENGONG':'LENGGONG','LAMGKAWI':'LANGKAWI',\n",
        "    'MARNG':'MARANG','MELAKA': 'MALACCA','MEKALA':'MALACCA','MANAMA BAHRAIN': 'MANAMA',\n",
        "    'NIBONG TEBA':'NIBONG TEBAL','NEW DELHI INDIA':'NEW DELHI','NEW DELH':'NEW DELHI','NARATHIWAT SOUTHERN THAILAND':'NARATHIWAT','MUNDOK SOUTHERN THAILAND':'MUNDOK',\n",
        "    'PARISBEIJING':'PARIS',\n",
        "    'PUTRAJAYAS': 'PUTRAJAYA','PUTRAYAJA': 'PUTRAJAYA','PUTRJAYA': 'PUTRAJAYA','PPUTRAJAYA': 'PUTRAJAYA','PATTANI THAILAND':'PATTANI','PASIR PUTIH':'PASIR PUTEH',\n",
        "    'PORT MORESBY PAPUA NEW GUINEA': 'PORT MORESBY','PANGKOR ISLAND':'PANGKOR','PULAU PERHENTIAN KECIL TERENGGANU':'PULAU PERHENTIAN',\n",
        "    'SEBERANG PERAI': 'SEBERANG PRAI','SUNNYLANDS CALIFORNIA': 'SUNNYLANDS','SUNGAI GOLOK THAILAND':'SUNGAI GOLOK',\n",
        "    'SUBANG': 'SUBANG JAYA','SONGKLA': 'SONGKHLA','SHAH  ALAM': 'SHAH ALAM','SEMENYEH': 'SEMENYIH','SELANGAU': 'SELANGOR','SARI': 'SARIKEI',\n",
        "    'SAMARAHAN': 'SAMARKAND','SADAO THAILAND': 'SADAO',   'ALSHAH ALAM': 'SHAH ALAM',\n",
        "    'THE HAGUE NETHERLANDS': 'THE HAGUE','TASHKENTL': 'TASHKENT','TAKBAI SOUTHERN THAILAND': 'TAKBAI','TAK': 'TAK THAILAND',\n",
        "    'VALLETTA MALTA': 'VALLETTA','VIENTIANE LAOS': 'VIENTIANE','VLADIVOSTOK RUSSIA': 'VLADIVOSTOK','VALETTA':'VALLETTA',\n",
        "    'ULAANBAATAR  MONGOLIA': 'ULAANBAATAR','ULAANBAATAR MONGOLIA': 'ULAANBAATAR','ULAANBAATAAR': 'ULAANBAATAR',\n",
        "    'WASHINGTON DC': 'WASHINGTON', 'KKUALA LUMPURR':'KUALA LUMPUR',\n",
        "\n",
        "}\n",
        "\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.upper()\n",
        "for old, new in place_corrections.items():\n",
        "    df_cleaned['Place'] = df_cleaned['Place'].str.replace(old, new, regex=False)\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.split(',').str[0]\n",
        "df_cleaned['Place'] = df_cleaned['Place'].str.replace(r'[^a-zA-Z\\s]+', '', regex=True)\n",
        "\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of articles per city\n",
        "city_counts = df_cleaned['Place'].value_counts()\n",
        "\n",
        "# Set a threshold: keep only cities with at least N articles\n",
        "threshold = 2\n",
        "valid_cities = city_counts[city_counts >= threshold].index\n",
        "\n",
        "# Save the valid cities to a CSV file\n",
        "pd.Series(valid_cities).to_csv('valid_cities.csv', index=False, header=False)\n",
        "\n",
        "# Filter the DataFrame to keep only valid cities\n",
        "df_cleaned = df_cleaned[df_cleaned['Place'].isin(valid_cities)]\n",
        "\n",
        "# Print processing metrics\n",
        "print(calculate_processing_metrics(df_cleaned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVArxFnNwEkC",
        "outputId": "d3317478-f260-4793-de22-de98db6b878e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0608 seconds\n",
            "Initial CPU Usage: 37.20%\n",
            "Final CPU Usage: 48.30%\n",
            "Memory Usage: 21.70%\n",
            "Throughput (Records per Second): 96862.92 records/sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to keep only rows where 'Place' is in valid_cities\n",
        "df_cleaned = df_cleaned[df_cleaned['Place'].isin(valid_cities)]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NukpGdvpwUch",
        "outputId": "8b0735c5-853b-4037-f0cb-9f8114b50353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0965 seconds\n",
            "Initial CPU Usage: 77.90%\n",
            "Final CPU Usage: 43.40%\n",
            "Memory Usage: 22.00%\n",
            "Throughput (Records per Second): 93712.48 records/sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QphApEUdjoBh"
      },
      "source": [
        "> **7. Extract Date from URL**\n",
        "\n",
        "We extract the date in `YYYY/MM` format from the URL and add it as a separate column in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae31be5-2e29-4986-db4b-5eaacec9ab7d",
        "id": "afgPEXH_joBh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date column extracted from the URL.\n",
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0009 seconds\n",
            "Initial CPU Usage: 57.80%\n",
            "Final CPU Usage: 4.10%\n",
            "Memory Usage: 21.00%\n",
            "Throughput (Records per Second): 102662.52 records/sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_cleaned['Date'] = df_cleaned['URL'].str.extract(r'(\\d{4}/\\d{2})')\n",
        "print(\"Date column extracted from the URL.\")\n",
        "print(calculate_processing_metrics(df_cleaned))  # Metrics after extracting the Date column\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFQgLlXHjoBi"
      },
      "source": [
        "> **8. Final Dataset**\n",
        "\n",
        "After cleaning and transforming the data, we earrange dataframe and export the cleaned dataset to a new CSV file (`finalData.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b78af26-c825-49f6-f2fb-657a8e4c9d6f",
        "id": "4HL93TksjoBi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows Processed: 102,756 records\n",
            "Total Processing Time: 1.0007 seconds\n",
            "Initial CPU Usage: 41.20%\n",
            "Final CPU Usage: 4.60%\n",
            "Memory Usage: 21.10%\n",
            "Throughput (Records per Second): 102684.34 records/sec\n"
          ]
        }
      ],
      "source": [
        "# Rearrange the columns to the desired order\n",
        "df_cleaned = df_cleaned[['Place', 'Date', 'Category', 'Title','Teaser']]\n",
        "print(calculate_processing_metrics(df_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufnl0cvCjoBi"
      },
      "outputs": [],
      "source": [
        "sorted_df = df_cleaned.sort_values(by=['Place'])\n",
        "sorted_df.to_csv('finalData.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB5SaNJwjoBi"
      },
      "source": []
    }
  ]
}