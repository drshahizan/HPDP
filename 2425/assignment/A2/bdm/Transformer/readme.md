# ðŸ“˜ Assignment 2: Mastering Big Data Handling

<table border="solid" align="center">
  <tr>
    <th>Name</th>
    <th>Matric Number</th>
  </tr>
  <tr>
    <td width=80%>TIEW CHUAN RONG</td>
    <td>A22EC0112</td>
  </tr>
  <tr>
    <td width=80%>DANIAL HARRIZ BIN MOHD ASINEH @MOHD ASNEH</td>
    <td>A22EC0152</td>
  </tr>
</table>
<br>

## Introduction
   In todayâ€™s digital world, companies collect a huge amount of data every day. So, it is important to select a efficient technique to handle this huge data. It is because with a correct tool we can load and process data faster and efficiently. This project is about learning how to deal with big data using real tools used by data engineer.
  
   For our project, we chose a large Transaction dataset thatâ€™s about 2.93MB in size. This dataset is selecting from kaggle with name ['ismetsemedov/transactions'](https://www.kaggle.com/datasets/ismetsemedov/transactions). Handling such a big dataset can be tricky because it can slow down the computer or even crash the program. So, we used several techniques to make it easier and faster to load, process, and analyze the data. These include breaking the data into chunks, loading only what we need, using lighter data types, sampling small parts, and using parallel processing tools like Dask and Polars. Other than that we also try using different library like Pandas, Polars and Dask to handle this huge amount of data. Then we made a comperative analysis on the processing time and memory usage for each tecniue used.

   Through this project, we gained hands-on experience in working with big data and learned how professional data engineers manage huge datasets efficiently.
  
## Library used
- Pandas
- Polars
- Dask
