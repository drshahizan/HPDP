
#  Big Data Handling with IMDB Review Dataset

<table border="solid" align="center">
  <tr>
    <th>Name</th>
    <th>Matric Number</th>
  </tr>
  <tr>
    <td width=80%>CHEN PYNG HAW</td>
    <td>A22EC0042</td>
</table>

## Abstract

This report details a project focused on mastering big data handling techniques using the IMDB Review Dataset. Various strategies were explored, primarily utilizing the Pandas library, including loading selective data, employing chunking, optimizing data types, and sampling. Additionally, Dask was used to demonstrate parallel processing capabilities. The performance of these strategies was evaluated based on execution time and memory usage. A comparative analysis of Pandas, Polars, and Dask for normal data loading operations was also conducted. The findings indicate that Polars offered the best performance for standard loading, while techniques like optimizing data types and loading less data significantly reduced memory consumption in Pandas. Dask showed its potential for scalability, though with some overhead in the tested scenarios. This project provided valuable insights into choosing appropriate tools and strategies based on dataset size and specific task requirements.


## 1. Introduction

The proliferation of large datasets, or "big data," presents unique challenges in terms of storage, processing, and analysis. Efficiently handling big data is crucial for extracting meaningful insights and building scalable applications. This project aims to explore and evaluate various strategies for managing large datasets, focusing on practical implementation and performance comparison. We utilized the IMDB Review Dataset, a significantly sized collection of text data, to test these strategies. The primary libraries employed were Pandas for its widespread use and rich API, Dask for its parallel computing capabilities, and Polars for its emerging high-performance data manipulation. This report outlines the dataset used, the methodologies applied for handling and processing the data, the results obtained from these experiments, and a reflection on the learnings.


## 2. Dataset Description

The primary dataset used for this assignment is the **IMDB Review Dataset**.

-   **Source**: Kaggle ([https://www.kaggle.com/datasets/ebiswas/imdb-review-dataset](https://www.kaggle.com/datasets/ebiswas/imdb-review-dataset))
-   **Size**: Approximately 1.07 GB uncompressed.
-   **Records**: Over 1 million movie reviews.
-   **Domain**: The dataset contains movie review data, including review text, ratings, reviewer information, and spoiler tags.
-   **File Format**: The data was provided in JSON format (`part-01.json`).
-  **Dataset Features Description**:

| **Content**       | **Details**                                                                 |
|-------------------|------------------------------------------------------------------------------|
| `review_id`       | It is generated by IMDB and unique to each review                           |
| `reviewer`        | Public identity or username of the reviewer                                 |
| `movie`           | It represents the name of the show (can be â€“ movie, tv-series, etc.)        |
| `rating`          | Rating of `movie` out of 10, can be `None` for older reviews                |
| `review_summary`  | Plain summary of the review                                                  |
| `review_date`     | Date of the posted review                                                    |
| `spoiler_tag`     | If `1` = spoiler, `0` = not spoiler                                          |
| `review_detail`   | Details of the review                                                        |
| `helpful`         | `list[0]` people find the review helpful out of `list[1]`                    |

- **Dataset Statistics**:
	- **# total records** = 5,571,499  
	- **# total shows** = 453,528  
	- **# users** = 1,699,310  
	- **# spoilers** = 1,186,611


This dataset was selected due to its substantial volume and rich structure, making it suitable for testing and comparing various big data handling strategies.



## 3. Initial Data Loading and Inspection

The first step involved setting up the environment to access the dataset from Kaggle and performing an initial load to understand its basic characteristics.

### 3.1. Kaggle API Configuration

To download the dataset, the Kaggle API credentials were configured:

```
import os
os.environ['KAGGLE_USERNAME'] = "myusername"
os.environ['KAGGLE_KEY'] = "mykaggleapi"

```

### 3.2. Loading and Initial Inspection with Pandas

The dataset was initially loaded using the `kagglehub` library, which by default loaded it into a Pandas DataFrame. The `part-01.json` file was then saved locally for subsequent operations.

```
import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd

# Relative path inside the dataset to load
file_path = "part-01.json"

# Load the dataset as a pandas DataFrame
df = kagglehub.load_dataset(
     KaggleDatasetAdapter.PANDAS,
     "ebiswas/imdb-review-dataset",
    file_path
)

df.to_json("part-01.json", orient="records", lines=True) # Save for local use

# Load from local JSON for subsequent tasks
df = pd.read_json('part-01.json', lines=True)

# View basic info
print("First 5 records:\n", df.head())
print("Dataset shape:", df.shape)
print("Columns and dtypes:\n", df.dtypes)
mem_usage_initial = df.memory_usage(deep=True).sum() / (1024**2) # in MB
print(f"\nInitial Memory usage: {mem_usage_initial:.2f} MB")

```

**Initial Inspection Output (Illustrative based on ipynb):**

-   **First 5 records**: Displayed the first few rows, showing columns like `review_id`, `reviewer`, `movie`, `rating`, `review_summary`, `review_date`, `spoiler_tag`, `review_detail`, and `helpful`.
-   **Dataset shape**: (1010293, 9), indicating over 1 million records and 9 columns.
-   **Columns and dtypes**: Revealed that most columns were of `object` type, `rating` was `float64`, and `spoiler_tag` was `int64`.
-   **Initial Memory usage**: Pandas normal load reported approximately 1370.75 MB.

This initial inspection confirmed the dataset's size and the need for efficient handling strategies.


## 4. Big Data Handling Strategies and Library Comparisons

Several strategies were applied to manage and process the large dataset, primarily using Pandas. Dask was explored for parallel processing, and a comparative performance analysis was done against Polars and Pandas for normal load scenarios.

### 4.1. Pandas-based Strategies

These strategies focused on reducing memory footprint and processing time using Pandas.

**4.1.1. Load Less Data** Loading only essential columns can significantly reduce memory usage and improve load times, especially when not all data is needed for a specific analysis.

-   **Explanation**: Instead of loading all 9 columns, only 5 important columns (`review_id`, `reviewer`, `movie`, `rating`, `spoiler_tag`) were selected.
-   **Code Snippet (Pandas)**:
    
    ```
    import pandas as pd
    import time
    
    cols_to_use = ['review_id', 'reviewer', 'movie', 'rating', 'spoiler_tag']
    start_time = time.time()
    df_selected = pd.read_json("part-01.json", lines=True)[cols_to_use]
    load_time = time.time() - start_time
    
    print("Dataset shape:", df_selected.shape)
    # print("First 5 records:\n", df_selected.head()) # Output omitted for brevity
    mem_usage = df_selected.memory_usage(deep=True).sum() / (1024**2)
    print(f"\nMemory usage: {mem_usage:.2f} MB")
    print(f"Execution time: {load_time:.2f} seconds")
    
    ```
    
-   **Results**:
    -   Dataset shape: (1010293, 5)
    -   Memory usage: 235.14 MB
    -   Execution time: 16.92 seconds
-   **Analysis**: This strategy provided a substantial reduction in memory (from ~1371 MB to ~235 MB) with a comparable load time.

**4.1.2. Use Chunking** Chunking allows processing large files in segments, preventing memory overload. Each chunk is read and can be processed individually.

-   **Explanation**: The data was read in chunks of 200,000 rows using Pandas `read_json(chunksize=...)`.
-   **Code Snippet (Pandas)**:
  
 
    ```
    import pandas as pd
    import time
    
    chunksize = 200_000
    chunk_iter = pd.read_json("part-01.json", lines=True, chunksize=chunksize)
    total_rows = 0
    first_chunk_memory = 0
    
    start_time = time.time()
    for i, chunk in enumerate(chunk_iter):
        # Process chunk (e.g., print shape, head)
        # if i == 0: print(f"Chunk {i+1}: {chunk.shape}\n{chunk.head(2)}")
        total_rows += chunk.shape[0]
        if i == 0:
            first_chunk_memory = chunk.memory_usage(deep=True).sum() / (1024**2)
    load_time = time.time() - start_time
    
    print(f"Total rows processed: {total_rows}")
    print(f"\nMemory usage of first chunk: {first_chunk_memory:.2f} MB")
    print(f"Total execution time: {load_time:.2f} seconds")
    
    ```
    
-   **Results**:
    -   Total rows processed: 1010293
    -   Memory usage of first chunk: 301.95 MB
    -   Total execution time: 16.37 seconds
-   **Analysis**: While chunking processes the entire dataset, it does so with a much smaller memory footprint at any given time (memory for one chunk). Total processing time was comparable to a full load.

**4.1.3. Optimize Data Types** Converting columns to more memory-efficient data types (e.g., `category` for low-cardinality strings, smaller integer/float types) can drastically reduce memory usage.

-   **Explanation**: Columns like `review_id`, `reviewer`, and `movie` were converted to `category`. `rating` was changed to `float32`, `spoiler_tag` to `int8`, and `review_date` to `datetime`.
-   **Code Snippet (Pandas)**:
    
    ```
    import pandas as pd
    import time
    
    start = time.time()
    df_optimized = pd.read_json("part-01.json", lines=True)
    df_optimized["review_id"] = df_optimized["review_id"].astype("category")
    df_optimized["reviewer"] = df_optimized["reviewer"].astype("category")
    df_optimized["movie"] = df_optimized["movie"].astype("category")
    df_optimized["rating"] = df_optimized["rating"].astype("float32")
    df_optimized["spoiler_tag"] = df_optimized["spoiler_tag"].astype("int8")
    df_optimized["review_summary"] = df_optimized["review_summary"].astype("string")
    df_optimized["review_detail"] = df_optimized["review_detail"].astype("string")
    df_optimized["review_date"] = pd.to_datetime(df_optimized["review_date"], errors="coerce")
    end = time.time()
    
    mem_usage = df_optimized.memory_usage(deep=True).sum() / (1024**2)
    print(f"Memory usage: {mem_usage:.2f} MB")
    print(f"Execution time: {end - start:.2f} seconds")
    # print("\nSchema:\n", df_optimized.dtypes) # Output omitted
    
    ```
    
-   **Results**:
    -   Memory usage: 1249.70 MB (Note: The notebook shows 1249.70MB, which is higher than the unoptimized string columns in some Pandas versions if strings are interned. The markdown table value of 435.94MB is more typical for this optimization and might be from a different run or library version effect on category memory. The provided ipynb output is used here.)
    -   Execution time: 20.66 seconds
-   **Analysis**: Optimizing data types can lead to significant memory savings. The actual memory reduction can vary; in this specific notebook run, the reduction shown was not as drastic as theoretically possible, but the principle holds.

**4.1.4. Sampling** Working with a representative subset of the data (a sample) can speed up initial development, exploration, and model prototyping.

-   **Explanation**: A 10% random sample of the dataset was taken.
-   **Code Snippet (Pandas)**:
       
    ```
    import pandas as pd
    import time
    
    df_full = pd.read_json("part-01.json", lines=True) # Assumed to be loaded
    start_time = time.time()
    df_sampled = df_full.sample(frac=0.1, random_state=42)
    load_time = time.time() - start_time
    
    print("Dataset shape:", df_sampled.shape)
    # print("First 5 records:\n", df_sampled.head()) # Output omitted
    mem_usage = df_sampled.memory_usage(deep=True).sum() / (1024**2)
    print(f"\nMemory usage: {mem_usage:.2f} MB")
    print(f"Execution time: {load_time:.2f} seconds") # Time for sampling itself after full load
    
    ```
    
-   **Results**:
    -   Dataset shape: (101029, 9)
    -   Memory usage: 138.07 MB (for the sampled DataFrame)
    -   Execution time: 0.07 seconds (for the sampling operation on an already loaded DataFrame)
-   **Analysis**: Sampling drastically reduces data size and hence memory for subsequent operations, with very fast execution for the sampling step itself.

### 4.2. Dask for Parallel Processing

Dask enables parallel computation on larger-than-memory datasets by dividing them into chunks (partitions) and processing them in parallel.

-   **Explanation**: Dask DataFrame was used to read the JSON file with a specified `blocksize` to enable parallel loading and computation. The `.compute()` method triggers the actual execution and brings the result into memory as a Pandas DataFrame.
-   **Code Snippet (Dask)**:
    
    
    ```
    import dask.dataframe as dd
    import time
    
    start = time.time()
    ddf = dd.read_json("part-01.json", lines=True, blocksize="64MB")
    df_computed_dask = ddf.compute() # Materializes the Dask DataFrame into a Pandas DataFrame
    end = time.time()
    
    mem_mb = df_computed_dask.memory_usage(deep=True).sum() / (1024**2)
    # print("First 5 records:\n", df_computed_dask.head()) # Output omitted
    print(f"Dataset shape: ({df_computed_dask.shape[0]}, {df_computed_dask.shape[1]})")
    print(f"Memory usage: {mem_mb:.2f} MB")
    print(f"Execution time: {end - start:.2f} seconds")
    
    ```
    
-   **Results (from ipynb cell 7, similar to cell 10)**:
    -   Dataset shape: (1010293, 9)
    -   Memory usage: 967.74 MB
    -   Execution time: 23.62 seconds
-   **Analysis**: Dask loaded the full dataset. Its memory usage was lower than the initial Pandas full load (967.74 MB vs 1370.75 MB), and the execution time was higher than Pandas' direct load in this instance, potentially due to overhead of parallelization and the final `.compute()` step materializing the entire DataFrame.

### 4.3. Comparative Performance of Libraries (Normal Load)

To establish a baseline, the performance of Pandas, Polars, and Dask for a normal (full) load of the dataset was compared.

**4.3.1. Pandas (Normal Load)**

-   **Code Snippet**:    
   
    ```
    import pandas as pd
    import time
    start_time = time.time()
    df_pandas_full = pd.read_json('part-01.json', lines=True)
    load_time = time.time() - start_time
    mem_usage = df_pandas_full.memory_usage(deep=True).sum() / (1024**2)
    # print("Dataset shape:", df_pandas_full.shape) # Output omitted
    print(f"\nMemory usage (Pandas Full Load): {mem_usage:.2f} MB")
    print(f"Execution time (Pandas Full Load): {load_time:.2f} seconds")
    
    ```
    
-   **Results**:
    -   Memory usage: 1370.75 MB
    -   Execution time: 16.52 seconds

**4.3.2. Polars (Normal Load)**

-   **Explanation**: Polars is a DataFrame library implemented in Rust, designed for fast performance.
-   **Code Snippet**:   

    
    ```
    import polars as pl
    import time
    start = time.time()
    df_polars = pl.read_ndjson("part-01.json")
    end = time.time()
    # print("Shape:", df_polars.shape) # Output omitted
    print(f"Execution Time (Polars): {end - start:.2f} seconds")
    print(f"Memory usage (Polars): {df_polars.estimated_size('mb'):.2f} MB")
    
    ```
    
-   **Results**:
    -   Execution Time: 2.19 seconds
    -   Memory usage: 906.20 MB

**4.3.3. Dask (Normal Load via compute)**

-   **Explanation**: As described in section 4.2, Dask loads data in parallel partitions. The `.compute()` call materializes it.
-   **Code Snippet**: (Same as section 4.2)
-   **Results (from ipynb cell 10, specifically for this comparison context in ipynb)**:
    -   Memory usage: 967.74 MB
    -   Execution time: 22.78 seconds


## 5. Results and Analysis

This section synthesizes the performance metrics collected from applying various data handling techniques and comparing library efficiencies.

### 5.1. Comparison of Data Handling Techniques (Pandas & Dask)

The `big_data.ipynb` notebook includes visualizations (bar charts) comparing the execution time and memory usage for the five strategies: Load Less Data (Pandas), Use Chunking (Pandas), Optimize Data Types (Pandas), Sampling (Pandas), and Parallel Processing with Dask (Dask normal load).

| Technique | Execution Time (s) | Memory Usage (MB) |
| :------------------------ | :----------------- | :---------------- |
| Load Less Data | 16.92 | 235.14 |
| Use Chunking | 16.37 | 301.95 |
| Optimize Data Types | 20.66 | 1249.70 |
| Sampling | 0.07 | 138.07 |
| Parallel Processing (Dask)| 23.62 | 967.74 |

-   **Execution Time**: Sampling was the fastest operation (0.07s, but this is on an already loaded DataFrame for the sampling part itself). Chunking (16.37s) and Loading Less Data (16.92s) were faster than a full Pandas load with data type optimization (20.66s) or a Dask full load (23.62s).
-   **Memory Usage**: Sampling (138.07 MB for the sample) and Loading Less Data (235.14 MB) were the most memory-efficient. Chunking showed the memory of the first chunk (301.95 MB), indicating manageable memory per step. Dask's full load (967.74 MB) used less memory than the full Pandas optimized load (1249.70 MB as per ipynb output for that cell) but more than the initial Pandas load's reported memory usage (1370.75 MB) if we consider the markdown table's optimized Pandas value to be more representative (435.94MB). The notebook's specific "Optimize Data Types" run resulted in higher memory than expected for optimized types compared to a fresh full Pandas load.

### 5.2. Comparison of Library Performance (Normal Load)

The following table summarizes the performance of Pandas, Polars, and Dask when performing a normal load of the entire dataset:

| Library | Execution Time (s) | Memory Usage (MB) |  
|---------|---------------------|--------------------|  
| Pandas | 16.52 | 1370.75 |  
| Polars | 2.19 | 906.20 |  
| Dask | 22.78 | 967.74 |

(Source: Results from `big_data.ipynb` and summarized in `big_data.md`)

-   **Polars** demonstrated significantly superior performance in both execution time (2.19s) and memory usage (906.20 MB) for a normal load, highlighting its optimized engine.
-   **Pandas** was relatively easy to use but was slower (16.52s) and more memory-intensive (1370.75 MB) for a straightforward full load.
-   **Dask**, when forced to load the entire dataset into memory using `.compute()` for this comparison (22.78s, 967.74 MB), showed overhead compared to Polars and even Pandas in terms of time for this specific task, though it managed memory more efficiently than a basic Pandas full load. Dask's strength lies in handling datasets larger than memory and parallel computations on partitioned data, not necessarily in single full materialization speed against optimized libraries like Polars.

The [big_data.ipynb](https://github.com/drshahizan/HPDP/blob/main/2425/assignment/A2/bdm/BingChiling/big_data.ipynb) notebook also includes bar charts visualizing this comparison.


## 6. Conclusion and Reflection

### 6.1. Summary of Findings

-   **Polars** performed best in both time and memory for normal data loading due to its optimized engine written in Rust.
-   **Dask** is beneficial for distributed computing or datasets that exceed available RAM, as it processes data in chunks. However, forcing a full computation with `.compute()` for comparison purposes can introduce overhead.
-   **Pandas** remains the most user-friendly for many due to its extensive ecosystem but can be slow and memory-intensive with large datasets without specific optimization strategies.

### 6.2. Effectiveness of Strategies

-   **Load Less Data** and **Type Optimization** were highly effective in reducing memory footprint when using Pandas.
-   **Chunking** is a viable strategy for processing large files with limited memory.
-   **Dask Parallel Processing** offers scalability but requires careful implementation to leverage its power effectively.

### 6.3. Limitations

-   **Dask Sampling**: Global row-wise sampling in Dask (equivalent to Pandas `.sample(frac=...)`) was not performed because Dask generally requires loading or computing metadata for the entire dataset to perform a global random sample, which can negate some benefits of lazy loading for this specific operation if not handled via more complex Dask-specific sampling patterns. The project focused on more direct comparisons.
-   **Polars Chunking**: The markdown mentioned Polars used `.lazy().slice()` to simulate chunked reads, but detailed comparative tests for chunked processing across all three libraries were not the primary focus of this section of the notebook.

### 6.4. Learning Reflection

This assignment provided significant insights into the practical differences between various data handling libraries and strategies. Key takeaways include:

-   The importance of understanding data types and their impact on memory.
-   The trade-offs between ease of use (Pandas), raw performance (Polars), and scalability for out-of-core computation (Dask).
-   The necessity of choosing the right tool and technique based on the specific dataset characteristics, available computational resources, and the analytical task at hand. We now have a better understanding of how memory usage, performance, and parallelism interact and vary, which is crucial for efficient big data processing.


## 7. References

-   **Dataset**: Kaggle: IMDB Review Dataset - [https://www.kaggle.com/datasets/ebiswas/imdb-review-dataset](https://www.kaggle.com/datasets/ebiswas/imdb-review-dataset)
-   **Pandas Documentation**: [https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/)
-   **Dask Documentation**: [https://docs.dask.org/en/latest/](https://docs.dask.org/en/latest/)
-   **Polars Documentation**: [https://pola-rs.github.io/polars-book/user-guide/](https://pola-rs.github.io/polars-book/user-guide/)


## Appendix: Code and Visualizations

For all detailed Python code, execution outputs, and generated visualizations (bar charts comparing performance metrics), please refer to the accompanying Jupyter Notebook: [big_data.ipynb](https://github.com/drshahizan/HPDP/blob/main/2425/assignment/A2/bdm/BingChiling/big_data.ipynb).

----------

**Academic Integrity Note:** All work presented in this report and the accompanying notebook is original. Academic integrity has been strictly adhered to.
