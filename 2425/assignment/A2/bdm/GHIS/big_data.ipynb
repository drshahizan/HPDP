{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEbxo5oOuyp8",
        "outputId": "6dfcc09a-434f-4658-b0e7-3b03766d063f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/archive_3.zip'\n",
        "extract_folder = '/content/github_issues/'\n",
        "\n",
        "!unzip -q \"{zip_path}\" -d \"{extract_folder}\""
      ],
      "metadata": {
        "id": "ceg5D_6pv0oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = extract_folder + 'github_issues.csv'"
      ],
      "metadata": {
        "id": "zmuuXt-xwB3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WpdgKzFsiCx",
        "outputId": "4ba53f5d-2f6d-4d5a-8e94-f5e1a4a54ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "file_path = '/content/github_issues/github_issues.csv'\n",
        "\n",
        "start_time = time.time()\n",
        "sample_data = pd.read_csv(file_path, nrows=100000)\n",
        "end_time = time.time()\n",
        "\n",
        "memory_bytes = sample_data.memory_usage(deep=True).sum()\n",
        "memory_megabytes = memory_bytes / (1024 ** 2)\n",
        "\n",
        "print(\"Sample Data Shape:\", sample_data.shape)\n",
        "print(\"\\nColumn Names:\", sample_data.columns.tolist())\n",
        "print(\"\\nData Types:\\n\", sample_data.dtypes)\n",
        "print(f\"\\nExecution time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Memory usage: {memory_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adL5gLk6osD4",
        "outputId": "d21f691c-0578-4042-8a41-ce18395705b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Data Shape: (100000, 3)\n",
            "\n",
            "Column Names: ['issue_url', 'issue_title', 'body']\n",
            "\n",
            "Data Types:\n",
            " issue_url      object\n",
            "issue_title    object\n",
            "body           object\n",
            "dtype: object\n",
            "\n",
            "Execution time: 1.7265 seconds\n",
            "Memory usage: 58.80 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Less Data\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "file_path = '/content/github_issues/github_issues.csv'  # update to your actual file path\n",
        "\n",
        "cols_to_load = ['issue_url', 'issue_title', 'body']\n",
        "\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(file_path, usecols=cols_to_load, nrows=100000)\n",
        "end_time = time.time()\n",
        "\n",
        "memory_bytes = df.memory_usage(deep=True).sum()\n",
        "memory_megabytes = memory_bytes / (1024 ** 2)\n",
        "\n",
        "print(\"Data Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\\n\", df.head())\n",
        "print(\"\\nData Types:\\n\", df.dtypes)\n",
        "print(f\"\\nExecution time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Memory usage: {memory_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waFS69duMsXQ",
        "outputId": "64415f78-8583-4cc9-f5ef-1b85e87a71b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape: (100000, 3)\n",
            "\n",
            "First 5 rows:\n",
            "                                            issue_url  \\\n",
            "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
            "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
            "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "\n",
            "                                         issue_title  \\\n",
            "0  can't load the addon. issue to: https://github...   \n",
            "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
            "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
            "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
            "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
            "\n",
            "                                                body  \n",
            "0  can't load the addon. issue to: https://github...  \n",
            "1  user experience: user who depends on screen re...  \n",
            "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
            "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "\n",
            "Data Types:\n",
            " issue_url      object\n",
            "issue_title    object\n",
            "body           object\n",
            "dtype: object\n",
            "\n",
            "Execution time: 0.9109 seconds\n",
            "Memory usage: 58.80 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chunking\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "csv_path = '/content/github_issues/github_issues.csv'  # Update with your actual path\n",
        "\n",
        "chunksize = 100_000  # Adjust if needed\n",
        "chunk_list = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(\n",
        "    csv_path,\n",
        "    encoding='utf-8',\n",
        "    engine='python',        # Use Python engine for tolerant parsing\n",
        "    on_bad_lines='skip',    # Skip bad lines to avoid crashing\n",
        "    chunksize=chunksize\n",
        ")):\n",
        "    print(f\"Processing chunk {i+1} with shape {chunk.shape}\")\n",
        "\n",
        "    if 'state' in chunk.columns:\n",
        "        filtered_chunk = chunk[chunk['state'] == 'open']\n",
        "    else:\n",
        "        filtered_chunk = chunk\n",
        "\n",
        "    chunk_list.append(filtered_chunk.head(100))\n",
        "\n",
        "    if i == 4:  # For demo, stop after 5 chunks\n",
        "        break\n",
        "\n",
        "df_sample = pd.concat(chunk_list)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "memory_bytes = df_sample.memory_usage(deep=True).sum()\n",
        "memory_megabytes = memory_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"\\nSample dataframe shape: {df_sample.shape}\")\n",
        "print(df_sample.head())\n",
        "\n",
        "print(f\"\\nExecution time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Memory usage: {memory_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "id": "aLOixokewEQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03e1c7d-9d67-41e7-d2f6-f5ff4d3545c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 1 with shape (100000, 3)\n",
            "Processing chunk 2 with shape (100000, 3)\n",
            "Processing chunk 3 with shape (100000, 3)\n",
            "Processing chunk 4 with shape (100000, 3)\n",
            "Processing chunk 5 with shape (100000, 3)\n",
            "\n",
            "Sample dataframe shape: (500, 3)\n",
            "                                           issue_url  \\\n",
            "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
            "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
            "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "\n",
            "                                         issue_title  \\\n",
            "0  can't load the addon. issue to: https://github...   \n",
            "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
            "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
            "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
            "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
            "\n",
            "                                                body  \n",
            "0  can't load the addon. issue to: https://github...  \n",
            "1  user experience: user who depends on screen re...  \n",
            "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
            "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "\n",
            "Execution time: 7.5677 seconds\n",
            "Memory usage: 0.52 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Optimize Data Types\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def optimize_dtypes(df):\n",
        "    cat_cols = ['state', 'author_association', 'repository_url']\n",
        "    for col in cat_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    int_cols = ['comments', 'id']\n",
        "    for col in int_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')\n",
        "\n",
        "    float_cols = []\n",
        "    for col in float_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Measure memory usage before optimization\n",
        "memory_before = df_sample.memory_usage(deep=True).sum() / (1024 ** 2)\n",
        "print(f\"Memory usage before optimization: {memory_before:.2f} MB\")\n",
        "\n",
        "start_time = time.time()\n",
        "df_sample = optimize_dtypes(df_sample)\n",
        "end_time = time.time()\n",
        "\n",
        "memory_after = df_sample.memory_usage(deep=True).sum() / (1024 ** 2)\n",
        "print(f\"Memory usage after optimization: {memory_after:.2f} MB\")\n",
        "print(f\"Optimization execution time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "print(\"\\nDataframe info after optimization:\")\n",
        "print(df_sample.info(memory_usage='deep'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvy_xA6Vphow",
        "outputId": "2a58185c-59ad-40cb-89b4-d56896154f51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage before optimization: 0.52 MB\n",
            "Memory usage after optimization: 0.52 MB\n",
            "Optimization execution time: 0.0003 seconds\n",
            "\n",
            "Dataframe info after optimization:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 500 entries, 0 to 400099\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   issue_url    500 non-null    object\n",
            " 1   issue_title  500 non-null    object\n",
            " 2   body         500 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 529.7 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampling\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Print original data shape\n",
        "print(\"Original dataframe shape:\", df_sample.shape)\n",
        "\n",
        "# --- RANDOM SAMPLING ---\n",
        "random_sample_frac = 0.2  # 20%\n",
        "\n",
        "start_time = time.time()\n",
        "random_sample = df_sample.sample(frac=random_sample_frac, random_state=42)\n",
        "end_time = time.time()\n",
        "\n",
        "memory_bytes_random = random_sample.memory_usage(deep=True).sum()\n",
        "memory_mb_random = memory_bytes_random / (1024 ** 2)\n",
        "\n",
        "print(f\"\\nRandom sample shape ({int(random_sample_frac*100)}%):\", random_sample.shape)\n",
        "print(\"Random sample preview:\")\n",
        "print(random_sample.head())\n",
        "print(f\"Random sampling execution time: {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Random sample memory usage: {memory_mb_random:.2f} MB\")\n",
        "\n",
        "# --- STRATIFIED SAMPLING ---\n",
        "\n",
        "stratify_col = 'state'  # Change this to your categorical column name\n",
        "\n",
        "if stratify_col in df_sample.columns:\n",
        "    print(\"\\nValue counts before filtering:\")\n",
        "    print(df_sample[stratify_col].value_counts())\n",
        "\n",
        "    valid_groups = df_sample[stratify_col].value_counts()[lambda x: x >= 2].index\n",
        "    df_filtered = df_sample[df_sample[stratify_col].isin(valid_groups)]\n",
        "\n",
        "    print(\"\\nData shape after filtering rare groups:\", df_filtered.shape)\n",
        "\n",
        "    start_time = time.time()\n",
        "    strat_sample, _ = train_test_split(\n",
        "        df_filtered,\n",
        "        test_size=0.8,\n",
        "        stratify=df_filtered[stratify_col],\n",
        "        random_state=42\n",
        "    )\n",
        "    end_time = time.time()\n",
        "\n",
        "    memory_bytes_strat = strat_sample.memory_usage(deep=True).sum()\n",
        "    memory_mb_strat = memory_bytes_strat / (1024 ** 2)\n",
        "\n",
        "    print(f\"\\nStratified sample shape ({int(random_sample_frac*100)}%):\", strat_sample.shape)\n",
        "    print(f\"Stratified sample value counts ({stratify_col}):\")\n",
        "    print(strat_sample[stratify_col].value_counts())\n",
        "    print(\"\\nStratified sample preview:\")\n",
        "    print(strat_sample.head())\n",
        "    print(f\"Stratified sampling execution time: {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Stratified sample memory usage: {memory_mb_strat:.2f} MB\")\n",
        "else:\n",
        "    print(f\"\\nColumn '{stratify_col}' not found in dataframe for stratified sampling.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbn-JYGapmA4",
        "outputId": "ede12c86-e7b3-4fe7-bae1-899f5b646932"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe shape: (500, 3)\n",
            "\n",
            "Random sample shape (20%): (100, 3)\n",
            "Random sample preview:\n",
            "                                                issue_url  \\\n",
            "300061  \"https://github.com/raksonibs/faker-elixir/iss...   \n",
            "73      \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "300074  \"https://github.com/momentumfrc/Scouting-Websi...   \n",
            "100055  \"https://github.com/Intel-bigdata/OAP/issues/255\"   \n",
            "100004  \"https://github.com/stavarengo/php-sigep/issue...   \n",
            "\n",
            "                                              issue_title  \\\n",
            "300061        can you please add a faker for filesystem ?   \n",
            "73      issue 2212: issue 2209: issue 2208: issue 2206...   \n",
            "300074        can't delete search bar contents in firefox   \n",
            "100055                         oap release 0.1 check list   \n",
            "100004                         nova etiqueta dos correios   \n",
            "\n",
            "                                                     body  \n",
            "300061  can you please add a faker module for filesyst...  \n",
            "73      gitlo = github x trello\\n---\\nthis board is no...  \n",
            "300074  the issue: - after entering a number, you cann...  \n",
            "100055  below is lists i can think of for now. i will ...  \n",
            "100004  alguem esta sabendo sobre o novo modelo de eti...  \n",
            "Random sampling execution time: 0.0011 seconds\n",
            "Random sample memory usage: 0.11 MB\n",
            "\n",
            "Column 'state' not found in dataframe for stratified sampling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parallel Processing with Dask\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import time\n",
        "\n",
        "csv_path = '/content/github_issues/github_issues.csv'\n",
        "cleaned_path = '/content/github_issues/github_issues_cleaned.csv'\n",
        "\n",
        "chunk_size = 100_000\n",
        "\n",
        "# Clean in chunks, write to cleaned_path\n",
        "start_time = time.time()\n",
        "with pd.read_csv(csv_path, engine='python', on_bad_lines='skip', encoding='utf-8', chunksize=chunk_size) as reader:\n",
        "    for i, chunk in enumerate(reader):\n",
        "        chunk.to_csv(cleaned_path, mode='a', index=False, header=(i==0))\n",
        "        print(f\"Cleaned chunk {i+1}\")\n",
        "cleaning_end_time = time.time()\n",
        "\n",
        "# Now read cleaned CSV with Dask\n",
        "cols_to_use = ['issue_url', 'issue_title', 'body']\n",
        "\n",
        "load_start_time = time.time()\n",
        "ddf = dd.read_csv(\n",
        "    cleaned_path,\n",
        "    usecols=cols_to_use,\n",
        "    dtype='object',\n",
        "    assume_missing=True,\n",
        "    encoding='utf-8',\n",
        "    blocksize='16MB'\n",
        ")\n",
        "load_end_time = time.time()\n",
        "\n",
        "# Trigger computation to load some data and estimate memory usage\n",
        "sample_df = ddf.head()  # triggers compute on first few rows\n",
        "memory_bytes = sample_df.memory_usage(deep=True).sum()\n",
        "memory_megabytes = memory_bytes / (1024 ** 2)\n",
        "\n",
        "print(\"\\nSample data from Dask dataframe:\")\n",
        "print(sample_df)\n",
        "\n",
        "print(f\"\\nChunk cleaning execution time: {cleaning_end_time - start_time:.4f} seconds\")\n",
        "print(f\"Dask data loading execution time: {load_end_time - load_start_time:.4f} seconds\")\n",
        "print(f\"Sample data memory usage: {memory_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM1ZsLPcprSY",
        "outputId": "cccd8a19-6231-4d0e-99b0-49c869415e6e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned chunk 1\n",
            "Cleaned chunk 2\n",
            "Cleaned chunk 3\n",
            "Cleaned chunk 4\n",
            "Cleaned chunk 5\n",
            "Cleaned chunk 6\n",
            "Cleaned chunk 7\n",
            "Cleaned chunk 8\n",
            "Cleaned chunk 9\n",
            "Cleaned chunk 10\n",
            "Cleaned chunk 11\n",
            "Cleaned chunk 12\n",
            "Cleaned chunk 13\n",
            "Cleaned chunk 14\n",
            "Cleaned chunk 15\n",
            "Cleaned chunk 16\n",
            "Cleaned chunk 17\n",
            "Cleaned chunk 18\n",
            "Cleaned chunk 19\n",
            "Cleaned chunk 20\n",
            "Cleaned chunk 21\n",
            "Cleaned chunk 22\n",
            "Cleaned chunk 23\n",
            "Cleaned chunk 24\n",
            "Cleaned chunk 25\n",
            "Cleaned chunk 26\n",
            "Cleaned chunk 27\n",
            "Cleaned chunk 28\n",
            "Cleaned chunk 29\n",
            "Cleaned chunk 30\n",
            "Cleaned chunk 31\n",
            "Cleaned chunk 32\n",
            "Cleaned chunk 33\n",
            "Cleaned chunk 34\n",
            "Cleaned chunk 35\n",
            "Cleaned chunk 36\n",
            "Cleaned chunk 37\n",
            "Cleaned chunk 38\n",
            "Cleaned chunk 39\n",
            "Cleaned chunk 40\n",
            "Cleaned chunk 41\n",
            "Cleaned chunk 42\n",
            "Cleaned chunk 43\n",
            "Cleaned chunk 44\n",
            "Cleaned chunk 45\n",
            "Cleaned chunk 46\n",
            "Cleaned chunk 47\n",
            "Cleaned chunk 48\n",
            "Cleaned chunk 49\n",
            "Cleaned chunk 50\n",
            "Cleaned chunk 51\n",
            "Cleaned chunk 52\n",
            "Cleaned chunk 53\n",
            "Cleaned chunk 54\n",
            "\n",
            "Sample data from Dask dataframe:\n",
            "                                           issue_url  \\\n",
            "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
            "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
            "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
            "\n",
            "                                         issue_title  \\\n",
            "0  can't load the addon. issue to: https://github...   \n",
            "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
            "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
            "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
            "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
            "\n",
            "                                                body  \n",
            "0  can't load the addon. issue to: https://github...  \n",
            "1  user experience: user who depends on screen re...  \n",
            "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "3  gitlo = github x trello\n",
            "---\n",
            "this board is now ...  \n",
            "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
            "\n",
            "Chunk cleaning execution time: 220.1132 seconds\n",
            "Dask data loading execution time: 0.1425 seconds\n",
            "Sample data memory usage: 0.01 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pandas\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "\n",
        "# File path\n",
        "csv_path = '/content/github_issues/github_issues.csv'\n",
        "\n",
        "# Monitoring setup\n",
        "monitoring = True\n",
        "performance_logs = []\n",
        "\n",
        "def monitor_performance(log_list, interval=0.5):\n",
        "    proc = psutil.Process()\n",
        "    while monitoring:\n",
        "        mem = proc.memory_info().rss / (1024*1024)  # MB\n",
        "        cpu = proc.cpu_percent(interval=None)      # %\n",
        "        log_list.append((time.time(), mem, cpu))\n",
        "        time.sleep(interval)\n",
        "\n",
        "monitor_thread = threading.Thread(target=monitor_performance, args=(performance_logs,))\n",
        "monitor_thread.start()\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"===== Pandas Processing Started =====\")\n",
        "\n",
        "# Load entire dataset (traditional)\n",
        "df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "print(f\"Loaded {len(df)} records.\")\n",
        "\n",
        "# Basic cleaning: fill missing, strip strings, drop duplicates\n",
        "df.fillna({'issue_title': 'No Title', 'body': ''}, inplace=True)\n",
        "for col in ['issue_title', 'body']:\n",
        "    df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "initial_len = len(df)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Removed {initial_len - len(df)} duplicate rows.\")\n",
        "\n",
        "monitoring = False\n",
        "monitor_thread.join()\n",
        "end_time = time.time()\n",
        "\n",
        "# Performance metrics\n",
        "total_time = end_time - start_time\n",
        "num_records = len(df)\n",
        "throughput = num_records / total_time\n",
        "peak_mem = max([m for _, m, _ in performance_logs])\n",
        "avg_mem = sum([m for _, m, _ in performance_logs]) / len(performance_logs)\n",
        "peak_cpu = max([c for _, _, c in performance_logs])\n",
        "avg_cpu = sum([c for _, _, c in performance_logs]) / len(performance_logs)\n",
        "\n",
        "print(\"\\n===== Pandas Performance Summary =====\")\n",
        "print(f\"Execution time: {total_time:.2f} seconds\")\n",
        "print(f\"Records processed: {num_records}\")\n",
        "print(f\"Throughput: {throughput:.2f} records/second\")\n",
        "print(f\"Average memory usage: {avg_mem:.2f} MB\")\n",
        "print(f\"Peak memory usage: {peak_mem:.2f} MB\")\n",
        "print(f\"Average CPU usage: {avg_cpu:.2f}%\")\n",
        "print(f\"Peak CPU usage: {peak_cpu:.2f}%\")\n",
        "print(\"Ease of Processing: Simple to use, widely known, but may be slow and memory intensive for very large datasets.\")\n",
        "print(\"======================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbTB7QNfqvtT",
        "outputId": "c565e714-1c39-4df2-a110-2aef58a3e276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Pandas Processing Started =====\n",
            "Loaded 5332153 records.\n",
            "Removed 5097 duplicate rows.\n",
            "\n",
            "===== Pandas Performance Summary =====\n",
            "Execution time: 87.15 seconds\n",
            "Records processed: 5327056\n",
            "Throughput: 61126.66 records/second\n",
            "Average memory usage: 4012.66 MB\n",
            "Peak memory usage: 5494.21 MB\n",
            "Average CPU usage: 98.10%\n",
            "Peak CPU usage: 224.50%\n",
            "Ease of Processing: Simple to use, widely known, but may be slow and memory intensive for very large datasets.\n",
            "======================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Polars\n",
        "import polars as pl\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "\n",
        "csv_path = '/content/github_issues/github_issues.csv'\n",
        "\n",
        "monitoring = True\n",
        "performance_logs = []\n",
        "\n",
        "def monitor_performance(log_list, interval=0.5):\n",
        "    proc = psutil.Process()\n",
        "    while monitoring:\n",
        "        mem = proc.memory_info().rss / (1024*1024)  # MB\n",
        "        cpu = proc.cpu_percent(interval=None)      # %\n",
        "        log_list.append((time.time(), mem, cpu))\n",
        "        time.sleep(interval)\n",
        "\n",
        "monitor_thread = threading.Thread(target=monitor_performance, args=(performance_logs,))\n",
        "monitor_thread.start()\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"===== Polars Processing Started =====\")\n",
        "\n",
        "df = pl.read_csv(csv_path)\n",
        "\n",
        "print(f\"Loaded {df.height} records.\")\n",
        "\n",
        "# Fill missing\n",
        "fill_dict = {'issue_title': 'No Title', 'body': ''}\n",
        "for col, val in fill_dict.items():\n",
        "    if col in df.columns:\n",
        "        df = df.with_columns(pl.col(col).fill_null(val))\n",
        "\n",
        "# Strip whitespace\n",
        "for col in ['issue_title', 'body']:\n",
        "    if col in df.columns:\n",
        "        df = df.with_columns(pl.col(col).str.strip_chars())\n",
        "\n",
        "# Drop duplicates\n",
        "initial_len = df.height\n",
        "df = df.unique()\n",
        "print(f\"Removed {initial_len - df.height} duplicate rows.\")\n",
        "\n",
        "monitoring = False\n",
        "monitor_thread.join()\n",
        "end_time = time.time()\n",
        "\n",
        "total_time = end_time - start_time\n",
        "num_records = df.height\n",
        "throughput = num_records / total_time\n",
        "peak_mem = max([m for _, m, _ in performance_logs])\n",
        "avg_mem = sum([m for _, m, _ in performance_logs]) / len(performance_logs)\n",
        "peak_cpu = max([c for _, _, c in performance_logs])\n",
        "avg_cpu = sum([c for _, _, c in performance_logs]) / len(performance_logs)\n",
        "\n",
        "print(\"\\n===== Polars Performance Summary =====\")\n",
        "print(f\"Execution time: {total_time:.2f} seconds\")\n",
        "print(f\"Records processed: {num_records}\")\n",
        "print(f\"Throughput: {throughput:.2f} records/second\")\n",
        "print(f\"Average memory usage: {avg_mem:.2f} MB\")\n",
        "print(f\"Peak memory usage: {peak_mem:.2f} MB\")\n",
        "print(f\"Average CPU usage: {avg_cpu:.2f}%\")\n",
        "print(f\"Peak CPU usage: {peak_cpu:.2f}%\")\n",
        "print(\"Ease of Processing: Fast and memory efficient, but requires learning Polars API.\")\n",
        "print(\"======================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u34iqSlWqlSF",
        "outputId": "391c0701-6eb7-4195-b57c-74a63d02605e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Polars Processing Started =====\n",
            "Loaded 5332153 records.\n",
            "Removed 5097 duplicate rows.\n",
            "\n",
            "===== Polars Performance Summary =====\n",
            "Execution time: 36.17 seconds\n",
            "Records processed: 5327056\n",
            "Throughput: 147290.82 records/second\n",
            "Average memory usage: 3790.27 MB\n",
            "Peak memory usage: 10736.56 MB\n",
            "Average CPU usage: 65.79%\n",
            "Peak CPU usage: 193.80%\n",
            "Ease of Processing: Fast and memory efficient, but requires learning Polars API.\n",
            "======================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PyArrow\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.csv as pv\n",
        "import pyarrow.compute as pc\n",
        "import time, psutil, threading\n",
        "\n",
        "csv_path = '/content/github_issues/github_issues.csv'\n",
        "cleaned_csv_path = '/content/github_issues_cleaned.csv'\n",
        "\n",
        "# Step 0: Clear existing cleaned file if it exists\n",
        "if os.path.exists(cleaned_csv_path):\n",
        "    os.remove(cleaned_csv_path)\n",
        "\n",
        "# Step 1: Clean CSV using pandas with sampling (RAM-friendly)\n",
        "print(\"===== Strict Pandas Cleaning with Sampling =====\")\n",
        "chunksize = 100000\n",
        "expected_columns = 3\n",
        "max_chunks = 10  # Process only first 10 chunks\n",
        "rows_per_chunk = 5000  # Sample size per chunk\n",
        "\n",
        "with pd.read_csv(\n",
        "    csv_path,\n",
        "    chunksize=chunksize,\n",
        "    encoding='utf-8',\n",
        "    engine='python',\n",
        "    on_bad_lines='skip',\n",
        "    quoting=1,  # QUOTE_ALL\n",
        "    skip_blank_lines=True\n",
        ") as reader:\n",
        "    for i, chunk in enumerate(reader):\n",
        "        chunk.dropna(axis=0, how='any', inplace=True)\n",
        "        if len(chunk.columns) > expected_columns:\n",
        "            chunk = chunk.iloc[:, :expected_columns]\n",
        "        chunk_sample = chunk.sample(min(rows_per_chunk, len(chunk)))\n",
        "        chunk_sample.to_csv(cleaned_csv_path, mode='a', index=False, header=(i == 0))\n",
        "        print(f\"Strictly cleaned and sampled chunk {i + 1}\")\n",
        "        if i + 1 >= max_chunks:\n",
        "            break\n",
        "\n",
        "# Step 2: Monitor system performance\n",
        "monitoring = True\n",
        "performance_logs = []\n",
        "\n",
        "def monitor_performance(log_list, interval=0.5):\n",
        "    proc = psutil.Process()\n",
        "    while monitoring:\n",
        "        mem = proc.memory_info().rss / (1024 * 1024)\n",
        "        cpu = proc.cpu_percent(interval=None)\n",
        "        log_list.append((time.time(), mem, cpu))\n",
        "        time.sleep(interval)\n",
        "\n",
        "monitor_thread = threading.Thread(target=monitor_performance, args=(performance_logs,))\n",
        "monitor_thread.start()\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"===== PyArrow Processing Started =====\")\n",
        "\n",
        "# Step 3: Load with PyArrow\n",
        "read_options = pv.ReadOptions(use_threads=True, block_size=5_000_000)\n",
        "parse_options = pv.ParseOptions(\n",
        "    delimiter=',',\n",
        "    quote_char='\"',\n",
        "    newlines_in_values=True,\n",
        "    invalid_row_handler=lambda row: 'skip'\n",
        ")\n",
        "convert_options = pv.ConvertOptions(strings_can_be_null=True)\n",
        "\n",
        "table = pv.read_csv(\n",
        "    cleaned_csv_path,\n",
        "    read_options=read_options,\n",
        "    parse_options=parse_options,\n",
        "    convert_options=convert_options\n",
        ")\n",
        "\n",
        "print(f\"Loaded {table.num_rows} records.\")\n",
        "\n",
        "# Step 4: Clean using PyArrow\n",
        "for col in ['issue_title', 'body']:\n",
        "    if col in table.column_names:\n",
        "        table = table.set_column(\n",
        "            table.schema.get_field_index(col),\n",
        "            col,\n",
        "            pc.utf8_trim(\n",
        "                pc.fill_null(table[col], 'No Title' if col == 'issue_title' else ''),\n",
        "                options=pc.TrimOptions(characters=' ')\n",
        "            )\n",
        "        )\n",
        "\n",
        "# Step 5: Convert to Pandas and drop duplicates\n",
        "df = table.to_pandas()\n",
        "initial_len = len(df)\n",
        "df = df.drop_duplicates()\n",
        "print(f\"Removed {initial_len - len(df)} duplicate rows.\")\n",
        "\n",
        "# Step 6: (Optional) Convert back to PyArrow table\n",
        "table = pa.Table.from_pandas(df)\n",
        "\n",
        "monitoring = False\n",
        "monitor_thread.join()\n",
        "end_time = time.time()\n",
        "\n",
        "# Step 7: Performance summary\n",
        "total_time = end_time - start_time\n",
        "num_records = table.num_rows\n",
        "throughput = num_records / total_time\n",
        "peak_mem = max(m for _, m, _ in performance_logs)\n",
        "avg_mem = sum(m for _, m, _ in performance_logs) / len(performance_logs)\n",
        "peak_cpu = max(c for _, _, c in performance_logs)\n",
        "avg_cpu = sum(c for _, _, c in performance_logs) / len(performance_logs)\n",
        "\n",
        "print(\"\\n===== PyArrow Performance Summary =====\")\n",
        "print(f\"Execution time: {total_time:.2f} seconds\")\n",
        "print(f\"Records processed: {num_records}\")\n",
        "print(f\"Throughput: {throughput:.2f} records/second\")\n",
        "print(f\"Average memory usage: {avg_mem:.2f} MB\")\n",
        "print(f\"Peak memory usage: {peak_mem:.2f} MB\")\n",
        "print(f\"Average CPU usage: {avg_cpu:.2f}%\")\n",
        "print(f\"Peak CPU usage: {peak_cpu:.2f}%\")\n",
        "print(\"Ease of Processing: Fast, memory-efficient, with seamless Pandas integration.\")\n",
        "print(\"======================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZEoL3rUrWIi",
        "outputId": "74dfa54a-4ad8-4268-eb34-786014b9d416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Strict Pandas Cleaning with Sampling =====\n",
            "Strictly cleaned and sampled chunk 1\n",
            "Strictly cleaned and sampled chunk 2\n",
            "Strictly cleaned and sampled chunk 3\n",
            "Strictly cleaned and sampled chunk 4\n",
            "Strictly cleaned and sampled chunk 5\n",
            "Strictly cleaned and sampled chunk 6\n",
            "Strictly cleaned and sampled chunk 7\n",
            "Strictly cleaned and sampled chunk 8\n",
            "Strictly cleaned and sampled chunk 9\n",
            "Strictly cleaned and sampled chunk 10\n",
            "===== PyArrow Processing Started =====\n",
            "Loaded 50000 records.\n",
            "Removed 2 duplicate rows.\n",
            "\n",
            "===== PyArrow Performance Summary =====\n",
            "Execution time: 1.68 seconds\n",
            "Records processed: 49998\n",
            "Throughput: 29707.18 records/second\n",
            "Average memory usage: 4858.99 MB\n",
            "Peak memory usage: 5846.79 MB\n",
            "Average CPU usage: 63.10%\n",
            "Peak CPU usage: 96.10%\n",
            "Ease of Processing: Fast, memory-efficient, with seamless Pandas integration.\n",
            "======================================\n"
          ]
        }
      ]
    }
  ]
}