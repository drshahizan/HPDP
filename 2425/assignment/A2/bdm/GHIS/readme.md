# 📊 Big Data Assignment 2: Project Overview

## 🎯 Project Title

**Efficient Big Data Handling with GitHub Issues Dataset**

## 👥 Team Members

- Member 1: LOW JIE SHENG A22EC0075 

---

## 📖 Project Description

This project investigates various strategies for handling large datasets effectively. Using the GitHub Issues Dataset (~2.85 GB), we implemented and benchmarked several methods across three popular Python libraries: **Pandas**, **Polars**, and **PyArrow**.

Key strategies examined include:

- Loading only required data columns  
- Processing data in chunks  
- Optimizing data types for memory savings  
- Sampling techniques  
- Parallel processing with Dask

---

## 📈 Performance Summary

Our evaluation focused on two main metrics: execution time and memory consumption. Full comparative analysis and visualizations are documented in the main report (`big_data.md`).

---

## 🛠️ Technologies & Libraries Used

- Python (Google Colab environment)  
- Pandas  
- Polars  
- PyArrow  
- Dask  
- Matplotlib (for charting and visualization)

---

## 📦 Dataset Information

- **Source:** [Kaggle - GitHub Issues Dataset](https://www.kaggle.com/datasets/davidshinn/github-issues/data)  
- **File Size:** Approximately 2.85 GB (CSV format)  
- **Content:** Software development / Issue tracking

---

## 🔗 Important Links

- 📄 [Main Report (big_data.md)](https://github.com/drshahizan/HPDP/blob/6b020bff5a057d9c5c09d566bfb5f315b3ebccdd/2425/assignment/A2/bdm/GHIS/big_data.md)  
- 📓 [Colab Notebook (big_data.ipynb)](https://github.com/drshahizan/HPDP/blob/6b020bff5a057d9c5c09d566bfb5f315b3ebccdd/2425/assignment/A2/bdm/GHIS/big_data.ipynb) 
