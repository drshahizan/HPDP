# Mastering Big Data Handling

## Brief Introduction

In today's data-centric landscape, organizations encounter significant challenges when managing and deriving insights from vast datasets. This assignment focuses on mastering big data handling using Python and its powerful libraries. Through practical experience, we will explore techniques such as chunking, sampling, type optimization, and parallel computing with Dask to efficiently handle datasets exceeding 700MB.

Our chosen dataset for this assignment is the [**NYC Parking Tickets 2017**](https://www.kaggle.com/datasets/new-york-city/nyc-parking-tickets?select=Parking_Violations_Issued_-_Fiscal_Year_2014__August_2013___June_2014_.csv), sourced from Kaggle. This dataset is not only substantial in size but also rich in content, allowing us to conduct thorough exploratory analysis and performance comparisons.

## Group Members
- Mulyani Binti Saripuddin
- Aliatul Izzah Binti Jasman

## Libraries Used
- Pandas
- Dask
- Polars

## Dataset Details
- **Data Source**: [Kaggle - NYC Parking Tickets](https://www.kaggle.com/datasets/new-york-city/nyc-parking-tickets?select=Parking_Violations_Issued_-_Fiscal_Year_2014__August_2013___June_2014_.csv)
- **Domain**: City Governance / Traffic Management
- **Number of Records**: 10,803,028 rows

This assignment will guide us through effective strategies for managing big data, ultimately enhancing our analytical skills and understanding of modern data processing challenges.
